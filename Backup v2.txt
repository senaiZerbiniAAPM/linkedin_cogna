# streamlit_thordata_sourcing.py
# -*- coding: utf-8 -*-
"""
Pesquisa de Perfis - Linkedin¬∑ Thordata SERP ‚Äî integrado (consulta -> tabela)

Funcionalidade principal:
- Executa consulta via Thordata / Scraper API
- Normaliza a resposta (desserializa se for string)
- Extrai lista 'organic' / 'data' e monta tabela com colunas:
  nome, Link de perfil, Local e descri√ß√£o
- Permite colar JSON ou subir arquivo JSON j√° salvo e montar a tabela diretamente
"""
import os
import time
import json
import re
from typing import Any, Dict, List, Optional, Tuple

import requests
import pandas as pd
import streamlit as st

st.set_page_config(page_title="Pesquisa de Perfis - Linkedin", layout="wide")

# ---------------- Config / endpoint ----------------
ENDPOINT = "https://scraperapi.thordata.com/request"

# ---------------- Utils / Heur√≠sticas ----------------
EMAIL_RE = re.compile(r'[\w\.-]+@[\w\.-]+\.\w+', re.I)

def extract_name_from_linkedin_title(title: Optional[str]) -> Optional[str]:
    """Heur√≠stica simples: corta antes de '|' e antes de '-' para obter nome."""
    if not title or not isinstance(title, str):
        return None
    s = title.split("|")[0]
    s = s.split(" - ")[0].split(" ‚Äî ")[0].strip()
    return s if s else None

def safe_json_load(obj: Any) -> Tuple[Optional[Dict], Optional[str]]:
    """
    Se obj for dict, retorna ele.
    Se obj for str, tenta json.loads e retorna dict.
    Se n√£o for poss√≠vel, retorna (None, mensagem_erro)
    """
    if isinstance(obj, dict):
        return obj, None
    if isinstance(obj, str):
        try:
            parsed = json.loads(obj)
            if isinstance(parsed, dict):
                return parsed, None
            return None, "JSON carregado n√£o √© um objeto dict (esperado)."
        except Exception as e:
            return None, f"Erro ao desserializar string JSON: {e}"
    return None, "Tipo de objeto inesperado; esperava dict ou str contendo JSON."

def find_organic_list(resp: Dict) -> List[Dict]:
    """
    Localiza a lista 'organic' no JSON retornado pela API Thordata / Scraper.
    Aceita varia√ß√µes de chaves e tamb√©m 'data' (TheirStack style).
    """
    if not isinstance(resp, dict):
        return []
    for key in ("organic", "organic_results", "results", "items"):
        v = resp.get(key)
        if isinstance(v, list):
            return v
    # fallback: some providers put results in 'data'
    v = resp.get("data")
    if isinstance(v, list):
        return v
    return []

def normalize_item_for_table(item: Dict) -> Dict[str, Optional[str]]:
    """
    Normaliza um item (dicion√°rio) retornado como 'organic' em tr√™s campos:
    nome, link de perfil, local e descri√ß√£o.
    """
    # obter t√≠tulo e poss√≠veis fontes
    title = (item.get("title") or item.get("job_title") or "") if isinstance(item, dict) else ""
    link = item.get("link") or item.get("url") or item.get("source_url") or item.get("final_url") or None
    description = (item.get("description") or item.get("snippet") or "") if isinstance(item, dict) else ""

    # heur√≠stica de nome
    name = extract_name_from_linkedin_title(title)
    if not name:
        # try source field (e.g., "LinkedIn ¬∑ Jo√£o Silva")
        source = item.get("source") or ""
        if isinstance(source, str) and "LinkedIn" in source:
            # after '¬∑' or ' - ' or '¬∑ '
            parts = re.split(r"¬∑|-", source)
            # normally the person's name is the last part
            if parts:
                candidate = parts[-1].strip()
                if candidate and len(candidate) > 1:
                    name = candidate

    if not name:
        # fallback: try first capitalized sequence from title (simple)
        m = re.search(r'\b([A-Z√Ä-≈∏][a-z√†-√ø]+(?:\s+[A-Z√Ä-≈∏][a-z√†-√ø]+){0,2})\b', title or "")
        if m:
            name = m.group(1)

    # Compose "Local e descri√ß√£o": keep description as-is (often contains local + text)
    local_desc = description or ""

    return {
        "nome": name or "",
        "Link de perfil": link or "",
        "Local e descri√ß√£o": local_desc or ""
    }

def resp_to_table(resp_obj: Any, max_rows: int = 10) -> Tuple[pd.DataFrame, Optional[str]]:
    """
    Converte a resposta da API (dict ou str) em um DataFrame com colunas:
    nome, Link de perfil, Local e descri√ß√£o
    Retorna (df, error_msg)
    """
    parsed, err = safe_json_load(resp_obj)
    if err:
        return pd.DataFrame(columns=["nome", "Link de perfil", "Local e descri√ß√£o"]), err

    organic_list = find_organic_list(parsed)
    rows = []
    for item in organic_list:
        # item pode j√° ter campos na forma esperada ou ser um job object
        row = normalize_item_for_table(item)
        rows.append(row)

    if not rows:
        # nenhuma entrada 'organic' encontrada: tentar heur√≠stica mais agressiva ‚Äî procurar por chaves dentro do dict
        # ex.: se existe 'organic' mas n√£o como lista, ou se a resposta foi aninhada de forma inesperada, tentamos extrair manualmente
        return pd.DataFrame(columns=["nome", "Link de perfil", "Local e descri√ß√£o"]), None

    df = pd.DataFrame(rows)
    # limitar a max_rows e reset index
    df = df.head(max_rows).reset_index(drop=True)
    return df, None

# ---------------- HTTP + Backoff (Thordata) ----------------
def exponential_backoff_sleep(attempt: int):
    wait = min(30, 2 ** attempt)
    time.sleep(wait)

def thordata_search(token: str,
                    q: str,
                    engine: str = "google",
                    domain: Optional[str] = None,
                    gl: Optional[str] = None,
                    hl: Optional[str] = None,
                    start: Optional[int] = None,
                    num: Optional[int] = None,
                    render_js: bool = False,
                    extra_params: Optional[Dict[str, Any]] = None,
                    max_retries: int = 6) -> Any:
    """
    Executa a chamada POST para Thordata. Retorna:
    - dict (carregado via resp.json()) quando status 200 e parse OK
    - string (resp.text) quando parse JSON falha
    - lan√ßa exce√ß√£o em erros cr√≠ticos
    """
    if not token:
        raise RuntimeError("Token n√£o informado. Defina THORDATA_TOKEN no ambiente ou cole na UI.")
    headers = {
        "Authorization": f"Bearer {token}",
        "Content-Type": "application/x-www-form-urlencoded"
    }
    data = {"engine": engine, "q": q, "json": "1"}
    if domain: data["domain"] = domain
    if gl: data["gl"] = gl
    if hl: data["hl"] = hl
    if start is not None: data["start"] = str(start)
    if num is not None: data["num"] = str(num)
    if render_js: data["render_js"] = "1"
    if extra_params:
        for k, v in (extra_params.items() if isinstance(extra_params, dict) else []):
            if v is not None:
                data[k] = str(v)

    attempt = 0
    while True:
        resp = requests.post(ENDPOINT, headers=headers, data=data, timeout=60)
        if resp.status_code == 200:
            try:
                return resp.json()
            except ValueError:
                return resp.text
        if resp.status_code == 429:
            if attempt >= max_retries:
                raise RuntimeError("Rate limited: excedeu tentativas (429).")
            exponential_backoff_sleep(attempt)
            attempt += 1
            continue
        if resp.status_code == 401:
            raise RuntimeError("401 Unauthorized - token inv√°lido ou expirado.")
        if resp.status_code == 402:
            raise RuntimeError("402 Payment Required - saldo insuficiente.")
        resp.raise_for_status()

# ---------------- UI ----------------
st.title("üîé   Pesquisa de Perfis - Linkedin")
st.markdown("Execute a busca; o resultado ser√° automaticamente estruturado em tabela (nome, Link de perfil, Local e descri√ß√£o). Tamb√©m √© poss√≠vel colar ou subir um JSON j√° salvo para montar a tabela diretamente.")

with st.sidebar:
    st.header("Configura√ß√µes API / Query")
    env_token = os.getenv("THORDATA_TOKEN", "")
    api_token = st.text_input("TheirData API Key (Bearer)", value=env_token, type="password",
                              help="Recomendado: defina THORDATA_TOKEN no ambiente.")
    st.markdown("---")
    st.header("Par√¢metros padr√£o")
    engine = st.selectbox("Mecanismo", options=["google", "bing"], index=0)
    domain = st.selectbox("Dom√≠nio Google", options=["google.com", "google.com.br", "google.co.uk"], index=0)
    gl = st.selectbox("Pa√≠s (gl)", options=["BR", "US", "CA", "UK", ""], index=0)
    hl = st.selectbox("Idioma (hl)", options=["pt-BR", "en", "pt", ""], index=0)
    render_js = st.checkbox("Renderizar JS (mais lento/custoso)", value=False)
    st.markdown("---")
    st.checkbox("Extrair nomes e e-mails automaticamente", value=True, key="do_extract_sidebar")

# formul√°rio de busca
with st.form("search_form"):
    st.subheader("Filtros de busca")
    area = st.selectbox("√Årea (ex.:)", ["Data Science", "Software Engineering", "DevOps", "Security", "Product", "Design", "Sales/Marketing", "Outro"], index=0)
    competence = st.text_input("Compet√™ncia / skill (ex.: Python, AWS, Spark)", placeholder="python, aws, spark")
    location = st.text_input("Localidade (cidade / estado / pa√≠s)", placeholder="S√£o Paulo, Brazil")
    free_text = st.text_input("Termos adicionais (ex.: 'Bacharel', 'Mestrado', 'S√™nior')", placeholder="")
    linkedin_only = st.checkbox("Somente LinkedIn (perfils) ‚Äî site:linkedin.com/in", value=False,
                                help="Se marcado, a query ser√° prefixada com site:linkedin.com/in OR site:linkedin.com/pub")
    per_page = st.slider("Resultados por p√°gina (limite para tabela)", min_value=5, max_value=50, value=10, step=5)
    page_idx = st.number_input("P√°gina (0 = primeira)", min_value=0, value=0, step=1)
    show_raw = st.checkbox("Mostrar JSON cru (ap√≥s consulta)", value=False)
    submitted = st.form_submit_button("üîé Pesquisar")

if "last_resp" not in st.session_state:
    st.session_state["last_resp"] = None
if "last_df" not in st.session_state:
    st.session_state["last_df"] = pd.DataFrame(columns=["nome", "Link de perfil", "Local e descri√ß√£o"])

# fun√ß√£o para montar query
def build_query(area: str, competence: str, location: str, free_text: str, linkedin_only: bool) -> str:
    parts = []
    if area and area != "Outro":
        parts.append(area)
    if competence:
        parts.append(competence)
    if location:
        parts.append(location)
    if free_text:
        parts.append(free_text)
    q = " ".join(parts).strip()
    if linkedin_only:
        if q:
            q = f"(site:linkedin.com/in OR site:linkedin.com/pub) {q}"
        else:
            q = "(site:linkedin.com/in OR site:linkedin.com/pub)"
    return q

# Quando o usu√°rio submete a busca
if submitted:
    token_to_use = api_token.strip() or os.getenv("THORDATA_TOKEN", "").strip()
    if not token_to_use:
        st.error("Token n√£o fornecido. Defina THORDATA_TOKEN no ambiente ou cole a chave no campo da lateral.")
    else:
        q = build_query(area, competence, location, free_text, linkedin_only)
        if not q:
            st.warning("Query vazia ‚Äî informe ao menos uma compet√™ncia, √°rea ou localidade.")
        else:
            start = page_idx * per_page
            with st.spinner("Consultando Thordata (SERP)..."):
                try:
                    resp_obj = thordata_search(token=token_to_use, q=q, engine=engine,
                                              domain=domain, gl=(gl or None), hl=(hl or None),
                                              start=start, num=per_page, render_js=render_js)
                except Exception as e:
                    st.error(f"Erro na busca: {e}")
                    resp_obj = None

            # armazenar resposta em session para usar na segunda etapa/estruturar
            st.session_state["last_resp"] = resp_obj

            # montar tabela
            if resp_obj is not None:
                df_table, err = resp_to_table(resp_obj, max_rows=per_page)
                if err:
                    st.warning(err)
                st.session_state["last_df"] = df_table
            else:
                st.session_state["last_df"] = pd.DataFrame(columns=["nome", "Link de perfil", "Local e descri√ß√£o"])

# area para colar JSON ou fazer upload (montar tabela sem consultar API)
st.markdown("---")
st.subheader("Ou: cole / carregue um JSON retornado pela API (opcional)")
col1, col2 = st.columns([3, 1])
with col1:
    pasted = st.text_area("Cole o JSON aqui (opcional)", height=140, placeholder='Cole aqui o JSON retornado pela API...')
with col2:
    upload = st.file_uploader("Ou fa√ßa upload do arquivo JSON", type=["json"])

if st.button("üîß Montar tabela a partir do JSON colado/subido"):
    # prioridade para upload
    content = None
    if upload is not None:
        try:
            raw = upload.read()
            # bytes -> str
            content = raw.decode("utf-8")
        except Exception as e:
            st.error(f"Erro lendo arquivo: {e}")
    elif pasted and pasted.strip():
        content = pasted.strip()

    if content:
        df_table, err = resp_to_table(content, max_rows=per_page)
        if err:
            st.warning(err)
        st.session_state["last_df"] = df_table
        # try to set last_resp to parsed json for raw view later
        parsed, jerr = safe_json_load(content)
        if parsed:
            st.session_state["last_resp"] = parsed
    else:
        st.info("Nenhum JSON fornecido para montar a tabela.")

# ----- Mostrar resultados estruturados (da √∫ltima resposta / upload) -----
st.markdown("---")
df_table = st.session_state.get("last_df", pd.DataFrame(columns=["nome", "Link de perfil", "Local e descri√ß√£o"]))
count = int(df_table.shape[0]) if hasattr(df_table, "shape") else 0
st.markdown(f"### Resultados estruturados ‚Äî {count} registros (mostrando at√© {per_page})")

if count == 0:
    st.info("Nenhum registro extra√≠do para a tabela ap√≥s limpeza heur√≠stica.")
else:
    # exibir tabela com colunas na ordem desejada
    display_df = df_table[["nome", "Link de perfil", "Local e descri√ß√£o"]].copy()
    # Reduzir comprimento das descri√ß√µes para a visualiza√ß√£o
    display_df["Local e descri√ß√£o"] = display_df["Local e descri√ß√£o"].astype(str).str.replace("\n", " ").str.slice(0, 500)
    st.dataframe(display_df, use_container_width=True)
    csv = display_df.to_csv(index=False).encode("utf-8")
    st.download_button("‚¨áÔ∏è Exportar CSV (perfis)", csv, file_name="sourcing_perfis.csv", mime="text/csv")

# mostrar links (clic√°veis)
if count > 0:
    st.markdown("### Links (clique para abrir)")
    for _, r in df_table.head(per_page).iterrows():
        nome = r.get("nome") or "(sem nome)"
        link = r.get("Link de perfil") or ""
        local_desc = (r.get("Local e descri√ß√£o") or "")[:200]
        if link:
            st.write(f"- [{nome}]({link}) ‚Äî {local_desc}")
        else:
            st.write(f"- {nome} ‚Äî {local_desc}")

# mostrar JSON cru se dispon√≠vel
if show_raw:
    resp_obj = st.session_state.get("last_resp")
    if resp_obj is None:
        st.info("Sem resposta em cache para exibir.")
    else:
        with st.expander("üîß JSON cru (data retornada pela API) ‚Äî expandir para inspecionar"):
            try:
                pretty = json.dumps(resp_obj, ensure_ascii=False, indent=2)
            except Exception:
                pretty = str(resp_obj)
            st.code(pretty[:20000], language="json")

st.markdown("---")
st.markdown(
    "**Aviso de Privacidade e Uso:** Coleta e tratamento de e-mails e nomes tem implica√ß√µes legais (LGPD/GDPR). "
    "Use estes dados apenas sob base legal apropriada, armazene com seguran√ßa e permita remo√ß√£o. "
    "Evite scraping massivo do LinkedIn; prefira ferramentas/licenciamento oficiais quando aplic√°vel."
)
