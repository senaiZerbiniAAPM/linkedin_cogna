# streamlit_thordata_sourcing_with_emails.py
# -*- coding: utf-8 -*-
"""
Pesquisa de Perfis - Linkedin ¬∑ Thordata SERP ‚Äî integrado (consulta -> tabela -> tentativa de e-mail)
Vers√£o com heur√≠sticas melhoradas e persist√™ncia (SQLite) quando o usu√°rio clicar em "Cadastrar".
"""
import os
import time
import json
import re
import hashlib
import html
import sqlite3
from typing import Any, Dict, List, Optional, Tuple
from urllib.parse import urlparse
from pathlib import Path
from datetime import datetime

import requests
import pandas as pd
import streamlit as st

st.set_page_config(page_title="Pesquisa de Perfis - Linkedin", layout="wide")

# ---------------- Config / endpoint / DB ----------------
ENDPOINT = "https://scraperapi.thordata.com/request"

# Database
DB_PATH = Path("./db/sourcing_profiles.db")
DB_PATH.parent.mkdir(parents=True, exist_ok=True)

# ---------------- Settings (adjust as needed) ----------------
CACHE_PATH = Path("./cache/email_cache.json")
CACHE_PATH.parent.mkdir(parents=True, exist_ok=True)
PER_CANDIDATE_DELAY = 0.8  # seconds between sub-requests per candidate
DEFAULT_MAX_QUERIES = 8
DEEP_MAX_QUERIES = 20
DEFAULT_RENDER_JS_FOR_PROFILE = True
USER_AGENT = "Mozilla/5.0 (compatible; ThordataBot/1.0)"

# ---------------- Utils / Heur√≠sticas ----------------
EMAIL_RE = re.compile(r'[\w\.-]+@[\w\.-]+\.\w+', re.I)

def init_db():
    """Cria a tabela se n√£o existir."""
    conn = sqlite3.connect(DB_PATH, timeout=30, check_same_thread=False)
    cur = conn.cursor()
    cur.execute("""
    CREATE TABLE IF NOT EXISTS sourcing_profiles (
        id INTEGER PRIMARY KEY AUTOINCREMENT,
        nome TEXT,
        profile_link TEXT,
        email TEXT,
        local_desc TEXT,
        created_at TEXT
    );
    """)
    conn.commit()
    return conn

# initialize DB on import
_conn_for_app = init_db()

def save_profiles_to_db(df: pd.DataFrame, conn: sqlite3.Connection) -> Tuple[int, int]:
    """
    Salva registros do DataFrame no DB.
    Evita duplicados verificando profile_link ou email j√° existente.
    Retorna (n_inseridos, n_ignorados).
    """
    if df is None or df.empty:
        return 0, 0

    inserted = 0
    ignored = 0
    cur = conn.cursor()
    now = datetime.utcnow().isoformat()
    with conn:
        for _, r in df.iterrows():
            nome = (r.get("nome") or "").strip()
            link = (r.get("Link de perfil") or "").strip()
            email = (r.get("Email") or "").strip()
            local_desc = (r.get("Local e descri√ß√£o") or "").strip()

            # basic duplicate check: same profile link OR same email (non-empty)
            dup_query = "SELECT 1 FROM sourcing_profiles WHERE profile_link = ?"
            cur.execute(dup_query, (link,))
            if cur.fetchone():
                ignored += 1
                continue
            if email:
                cur.execute("SELECT 1 FROM sourcing_profiles WHERE email = ?", (email,))
                if cur.fetchone():
                    ignored += 1
                    continue

            # insert
            cur.execute(
                "INSERT INTO sourcing_profiles (nome, profile_link, email, local_desc, created_at) VALUES (?, ?, ?, ?, ?)",
                (nome or None, link or None, email or None, local_desc or None, now)
            )
            inserted += 1
    return inserted, ignored

def fetch_all_profiles(conn: sqlite3.Connection) -> pd.DataFrame:
    cur = conn.cursor()
    cur.execute("SELECT id, nome, profile_link, email, local_desc, created_at FROM sourcing_profiles ORDER BY id DESC")
    rows = cur.fetchall()
    cols = ["id", "nome", "Link de perfil", "Email", "Local e descri√ß√£o", "created_at"]
    return pd.DataFrame(rows, columns=cols)

# --------- (The rest of the app: Thordata search + email heuristics) ---------
def extract_name_from_linkedin_title(title: Optional[str]) -> Optional[str]:
    if not title or not isinstance(title, str):
        return None
    s = title.split("|")[0]
    s = s.split(" - ")[0].split(" ‚Äî ")[0].strip()
    return s if s else None

def safe_json_load(obj: Any) -> Tuple[Optional[Dict], Optional[str]]:
    if isinstance(obj, dict):
        return obj, None
    if isinstance(obj, str):
        try:
            parsed = json.loads(obj)
            if isinstance(parsed, dict):
                return parsed, None
            return None, "JSON carregado n√£o √© um objeto dict (esperado)."
        except Exception as e:
            return None, f"Erro ao desserializar string JSON: {e}"
    return None, "Tipo de objeto inesperado; esperava dict ou str contendo JSON."

def find_organic_list(resp: Dict) -> List[Dict]:
    if not isinstance(resp, dict):
        return []
    for key in ("organic", "organic_results", "results", "items"):
        v = resp.get(key)
        if isinstance(v, list):
            return v
    v = resp.get("data")
    if isinstance(v, list):
        return v
    return []

def normalize_item_for_table(item: Dict) -> Dict[str, Optional[str]]:
    title = (item.get("title") or item.get("job_title") or "") if isinstance(item, dict) else ""
    link = item.get("link") or item.get("url") or item.get("source_url") or item.get("final_url") or None
    description = (item.get("description") or item.get("snippet") or "") if isinstance(item, dict) else ""

    name = extract_name_from_linkedin_title(title)
    if not name:
        source = item.get("source") or ""
        if isinstance(source, str) and "LinkedIn" in source:
            parts = re.split(r"¬∑|-", source)
            if parts:
                candidate = parts[-1].strip()
                if candidate and len(candidate) > 1:
                    name = candidate
    if not name:
        m = re.search(r'\b([A-Z√Ä-≈∏][a-z√†-√ø]+(?:\s+[A-Z√Ä-≈∏][a-z√†-√ø]+){0,2})\b', title or "")
        if m:
            name = m.group(1)
    local_desc = description or ""
    return {
        "nome": name or "",
        "Link de perfil": link or "",
        "Email": "",
        "Local e descri√ß√£o": local_desc or ""
    }

def resp_to_table(resp_obj: Any, max_rows: int = 10) -> Tuple[pd.DataFrame, Optional[str]]:
    parsed, err = safe_json_load(resp_obj)
    if err:
        return pd.DataFrame(columns=["nome", "Link de perfil", "Email", "Local e descri√ß√£o"]), err

    organic_list = find_organic_list(parsed)
    rows = []
    for item in organic_list:
        row = normalize_item_for_table(item)
        rows.append(row)

    if not rows:
        return pd.DataFrame(columns=["nome", "Link de perfil", "Email", "Local e descri√ß√£o"]), None

    df = pd.DataFrame(rows)
    df = df.head(max_rows).reset_index(drop=True)
    return df, None

def exponential_backoff_sleep(attempt: int):
    wait = min(30, 2 ** attempt)
    time.sleep(wait)

def thordata_search(token: str,
                    q: str,
                    engine: str = "google",
                    domain: Optional[str] = None,
                    gl: Optional[str] = None,
                    hl: Optional[str] = None,
                    start: Optional[int] = None,
                    num: Optional[int] = None,
                    render_js: bool = False,
                    extra_params: Optional[Dict[str, Any]] = None,
                    max_retries: int = 6) -> Any:
    if not token:
        raise RuntimeError("Token n√£o informado. Defina THORDATA_TOKEN no ambiente ou cole na UI.")
    headers = {
        "Authorization": f"Bearer {token}",
        "Content-Type": "application/x-www-form-urlencoded"
    }
    data = {"engine": engine, "q": q, "json": "1"}
    if domain: data["domain"] = domain
    if gl: data["gl"] = gl
    if hl: data["hl"] = hl
    if start is not None: data["start"] = str(start)
    if num is not None: data["num"] = str(num)
    if render_js: data["render_js"] = "1"
    if extra_params:
        for k, v in (extra_params.items() if isinstance(extra_params, dict) else []):
            if v is not None:
                data[k] = str(v)

    attempt = 0
    while True:
        resp = requests.post(ENDPOINT, headers=headers, data=data, timeout=60)
        if resp.status_code == 200:
            try:
                return resp.json()
            except ValueError:
                return resp.text
        if resp.status_code == 429:
            if attempt >= max_retries:
                raise RuntimeError("Rate limited: excedeu tentativas (429).")
            exponential_backoff_sleep(attempt)
            attempt += 1
            continue
        if resp.status_code == 401:
            raise RuntimeError("401 Unauthorized - token inv√°lido ou expirado.")
        if resp.status_code == 402:
            raise RuntimeError("402 Payment Required - saldo insuficiente.")
        resp.raise_for_status()

# ---------------- Email search helpers (improved) ----------------
def _load_cache_persisted() -> Dict[str, str]:
    try:
        if CACHE_PATH.exists():
            with open(CACHE_PATH, "r", encoding="utf-8") as f:
                return json.load(f)
    except Exception:
        pass
    return {}

def _save_cache_persisted(cache: Dict[str, str]):
    try:
        with open(CACHE_PATH, "w", encoding="utf-8") as f:
            json.dump(cache, f, ensure_ascii=False, indent=2)
    except Exception:
        pass

def _cache_get(key: str):
    if "_email_cache" not in st.session_state:
        st.session_state["_email_cache"] = _load_cache_persisted()
    return st.session_state["_email_cache"].get(key)

def _cache_set(key: str, value):
    if "_email_cache" not in st.session_state:
        st.session_state["_email_cache"] = _load_cache_persisted()
    st.session_state["_email_cache"][key] = value or ""
    _save_cache_persisted(st.session_state["_email_cache"])

def extract_emails_from_obj(obj: Any) -> List[str]:
    found = set()
    try:
        if isinstance(obj, str):
            for m in EMAIL_RE.findall(obj):
                found.add(m.lower())
            return list(found)
        if isinstance(obj, dict):
            for key in ("snippet", "description", "title", "content", "text"):
                v = obj.get(key)
                if isinstance(v, str):
                    for m in EMAIL_RE.findall(v):
                        found.add(m.lower())
            for k, v in obj.items():
                if isinstance(v, str):
                    for m in EMAIL_RE.findall(v):
                        found.add(m.lower())
                elif isinstance(v, list):
                    for item in v:
                        if isinstance(item, str):
                            for m in EMAIL_RE.findall(item):
                                found.add(m.lower())
                        elif isinstance(item, dict):
                            for kk in ("snippet","description","title","url","link"):
                                vv = item.get(kk)
                                if isinstance(vv, str):
                                    for m in EMAIL_RE.findall(vv):
                                        found.add(m.lower())
    except Exception:
        pass
    return list(found)

def try_fetch_page_and_extract_emails(url: str, token: str, render_js: bool = False) -> List[str]:
    emails = set()
    if not url:
        return []
    try:
        parsed = urlparse(url)
        domain = parsed.netloc.lower()
    except Exception:
        domain = ""
    try:
        if "linkedin.com" in domain:
            try:
                resp = thordata_search(token=token, q=url, engine="google", render_js=True, num=1)
            except Exception:
                resp = None
            if resp:
                emails.update(extract_emails_from_obj(resp))
                organics = find_organic_list(resp) if isinstance(resp, dict) else []
                for item in organics:
                    for f in ("snippet","description","title","link","url"):
                        vv = item.get(f)
                        if isinstance(vv, str):
                            for m in EMAIL_RE.findall(vv):
                                emails.add(m.lower())
        else:
            headers = {"User-Agent": USER_AGENT}
            try:
                r = requests.get(url, timeout=15, headers=headers)
                if r.status_code == 200 and r.text:
                    text = html.unescape(r.text)
                    for m in EMAIL_RE.findall(text):
                        emails.add(m.lower())
            except Exception:
                pass
    except Exception:
        pass
    return sorted(emails)

def build_domain_priority_query(name: str, location: str = "") -> List[str]:
    qbase = []
    if name:
        qbase.append(f'"{name}"')
    if location:
        qbase.append(f'"{location}"')
    joined = " ".join(qbase).strip()
    queries = []
    if joined:
        queries.append(f'{joined} (email OR "e-mail" OR contato OR contato@ OR "@")')
        queries.append(f'(site:linkedin.com/in OR site:linkedin.com/pub) {joined} (contact OR "contato" OR email)')
        dev_sites = ["github.com", "gitlab.com", "github.io", "medium.com", "kaggle.com", "stackoverflow.com", "dev.to"]
        for ds in dev_sites:
            queries.append(f'site:{ds} {joined} (email OR contato OR "@")')
        queries.append(f'{joined} (site:.com OR site:.dev OR site:.site OR site:.me OR site:.blog) (email OR contato OR "@")')
        queries.append(f'(site:twitter.com OR site:facebook.com OR site:instagram.com) {joined} ("@" OR contato OR email)')
    else:
        queries.append('(email OR "e-mail" OR contato OR "@")')
    return queries

def find_email_for_name_v2(token: str,
                           name: str,
                           location: str = "",
                           profile_link: Optional[str] = None,
                           engine: str = "google",
                           domain: Optional[str] = None,
                           gl: Optional[str] = None,
                           hl: Optional[str] = None,
                           render_js: bool = False,
                           max_queries_per_candidate: int = DEFAULT_MAX_QUERIES) -> Optional[str]:
    if not name and not profile_link:
        return None
    key_src = f"{name}|{location}|{profile_link}"
    key = hashlib.sha1(key_src.encode("utf-8")).hexdigest()
    cached = _cache_get(key)
    if cached is not None:
        return cached or None
    found_emails: List[str] = []
    try:
        if profile_link:
            try:
                emails = try_fetch_page_and_extract_emails(profile_link, token=token, render_js=DEFAULT_RENDER_JS_FOR_PROFILE)
                if emails:
                    found_emails.extend(emails)
            except Exception:
                pass
            time.sleep(PER_CANDIDATE_DELAY)
        queries = build_domain_priority_query(name, location)
        queries = queries[:max_queries_per_candidate]
        for q in queries:
            try:
                resp = thordata_search(token=token, q=q, engine=engine, domain=domain, gl=gl, hl=hl, render_js=render_js, num=10)
            except Exception:
                resp = None
            if resp:
                if isinstance(resp, dict):
                    found_emails.extend(extract_emails_from_obj(resp))
                    organics = find_organic_list(resp)
                    for item in organics:
                        for f in ("snippet","description","title","link","url","source_url"):
                            vv = item.get(f)
                            if isinstance(vv, str):
                                for m in EMAIL_RE.findall(vv):
                                    found_emails.append(m.lower())
                        link = item.get("link") or item.get("url") or item.get("source_url")
                        if link and "linkedin.com" not in (link or "").lower():
                            try:
                                page_emails = try_fetch_page_and_extract_emails(link, token=token, render_js=False)
                                if page_emails:
                                    found_emails.extend(page_emails)
                            except Exception:
                                pass
                elif isinstance(resp, str):
                    for m in EMAIL_RE.findall(resp):
                        found_emails.append(m.lower())
            found_emails = list(dict.fromkeys([e.lower() for e in found_emails if e]))
            if found_emails:
                _cache_set(key, found_emails[0])
                return found_emails[0]
            time.sleep(PER_CANDIDATE_DELAY)
    except Exception:
        pass
    _cache_set(key, "")
    return None

def enrich_df_with_emails(df: pd.DataFrame, token: str, engine: str, domain: Optional[str],
                          gl: Optional[str], hl: Optional[str], render_js: bool,
                          max_per_candidate: int = DEFAULT_MAX_QUERIES, force_deep: bool = False) -> pd.DataFrame:
    if df is None or df.empty:
        return df
    if "Email" not in df.columns:
        df["Email"] = ""
    total = df.shape[0]
    progress = st.progress(0)
    for i, row in df.iterrows():
        if row.get("Email"):
            progress.progress(int((i+1)/total*100))
            continue
        name = (row.get("nome") or "").strip()
        local = (row.get("Local e descri√ß√£o") or "")
        profile_link = row.get("Link de perfil") or ""
        depth = DEEP_MAX_QUERIES if force_deep else max_per_candidate
        try:
            email = find_email_for_name_v2(token=token, name=name, location=local, profile_link=profile_link,
                                           engine=engine, domain=domain, gl=gl, hl=hl, render_js=render_js,
                                           max_queries_per_candidate=depth)
        except Exception:
            email = None
        if email:
            df.at[i, "Email"] = email
        else:
            df.at[i, "Email"] = ""
        time.sleep(PER_CANDIDATE_DELAY)
        progress.progress(int((i+1)/total*100))
    progress.empty()
    return df

# ---------------- UI ----------------
st.title("üîé   Pesquisa de Perfis - Linkedin")
st.markdown("Execute a busca; o resultado ser√° automaticamente estruturado em tabela (nome, Link de perfil, Email, Local e descri√ß√£o).")

with st.sidebar:
    st.header("Configura√ß√µes API / Query")
    env_token = os.getenv("THORDATA_TOKEN", "")
    api_token = st.text_input("TheirData API Key (Bearer)", value=env_token, type="password",
                              help="Recomendado: defina THORDATA_TOKEN no ambiente.")
    st.markdown("---")
    st.header("Par√¢metros padr√£o")
    engine = st.selectbox("Mecanismo", options=["google", "bing"], index=0)
    domain = st.selectbox("Dom√≠nio Google", options=["google.com", "google.com.br", "google.co.uk"], index=0)
    gl = st.selectbox("Pa√≠s (gl)", options=["BR", "US", "CA", "UK", ""], index=0)
    hl = st.selectbox("Idioma (hl)", options=["pt-BR", "en", "pt", ""], index=0)
    render_js = st.checkbox("Renderizar JS (mais lento/custoso)", value=False)
    st.markdown("---")
    do_extract = st.checkbox("Extrair nomes e e-mails automaticamente", value=True, key="do_extract_sidebar")
    st.markdown("Cache e limites")
    st.write(f"Cache persistente: `{CACHE_PATH.resolve()}`")
    st.write("Ajuste PER_CANDIDATE_DELAY / MAX_QUERIES no topo do arquivo se necess√°rio.")

with st.form("search_form"):
    st.subheader("Filtros de busca")
    area = st.selectbox("√Årea (ex.:)", ["Data Science", "Software Engineering", "DevOps", "Security", "Product", "Design", "Sales/Marketing", "Outro"], index=0)
    competence = st.text_input("Compet√™ncia / skill (ex.: Python, AWS, Spark)", placeholder="python, aws, spark")
    location = st.text_input("Localidade (cidade / estado / pa√≠s)", placeholder="S√£o Paulo, Brazil")
    free_text = st.text_input("Termos adicionais (ex.: 'Bacharel', 'Mestrado', 'S√™nior')", placeholder="")
    linkedin_only = st.checkbox("Somente LinkedIn (perfils) ‚Äî site:linkedin.com/in", value=False,
                                help="Se marcado, a query ser√° prefixada com site:linkedin.com/in OR site:linkedin.com/pub")
    per_page = st.slider("Resultados por p√°gina (limite para tabela)", min_value=5, max_value=50, value=10, step=5)
    page_idx = st.number_input("P√°gina (0 = primeira)", min_value=0, value=0, step=1)
    show_raw = st.checkbox("Mostrar JSON cru (ap√≥s consulta)", value=False)
    submitted = st.form_submit_button("üîé Pesquisar")

if "last_resp" not in st.session_state:
    st.session_state["last_resp"] = None
if "last_df" not in st.session_state:
    st.session_state["last_df"] = pd.DataFrame(columns=["nome", "Link de perfil", "Email", "Local e descri√ß√£o"])

def build_query(area: str, competence: str, location: str, free_text: str, linkedin_only: bool) -> str:
    parts = []
    if area and area != "Outro":
        parts.append(area)
    if competence:
        parts.append(competence)
    if location:
        parts.append(location)
    if free_text:
        parts.append(free_text)
    q = " ".join(parts).strip()
    if linkedin_only:
        if q:
            q = f"(site:linkedin.com/in OR site:linkedin.com/pub) {q}"
        else:
            q = "(site:linkedin.com/in OR site:linkedin.com/pub)"
    return q

# search submit handling
if submitted:
    token_to_use = api_token.strip() or os.getenv("THORDATA_TOKEN", "").strip()
    if not token_to_use:
        st.error("Token n√£o fornecido. Defina THORDATA_TOKEN no ambiente ou cole a chave no campo da lateral.")
    else:
        q = build_query(area, competence, location, free_text, linkedin_only)
        if not q:
            st.warning("Query vazia ‚Äî informe ao menos uma compet√™ncia, √°rea ou localidade.")
        else:
            start = page_idx * per_page
            with st.spinner("Consultando Thordata (SERP)..."):
                try:
                    resp_obj = thordata_search(token=token_to_use, q=q, engine=engine,
                                              domain=domain, gl=(gl or None), hl=(hl or None),
                                              start=start, num=per_page, render_js=render_js)
                except Exception as e:
                    st.error(f"Erro na busca: {e}")
                    resp_obj = None

            st.session_state["last_resp"] = resp_obj

            if resp_obj is not None:
                df_table, err = resp_to_table(resp_obj, max_rows=per_page)
                if err:
                    st.warning(err)
                st.session_state["last_df"] = df_table
            else:
                st.session_state["last_df"] = pd.DataFrame(columns=["nome", "Link de perfil", "Email", "Local e descri√ß√£o"])

# area para colar JSON ou fazer upload (montar tabela sem consultar API)
st.markdown("---")
st.subheader("Ou: cole / carregue um JSON retornado pela API (opcional)")
col1, col2 = st.columns([3, 1])
with col1:
    pasted = st.text_area("Cole o JSON aqui (opcional)", height=140, placeholder='Cole aqui o JSON retornado pela API...')
with col2:
    upload = st.file_uploader("Ou fa√ßa upload do arquivo JSON", type=["json"])

if st.button("üîß Montar tabela a partir do JSON colado/subido"):
    content = None
    if upload is not None:
        try:
            raw = upload.read()
            content = raw.decode("utf-8")
        except Exception as e:
            st.error(f"Erro lendo arquivo: {e}")
    elif pasted and pasted.strip():
        content = pasted.strip()

    if content:
        df_table, err = resp_to_table(content, max_rows=per_page)
        if err:
            st.warning(err)
        st.session_state["last_df"] = df_table
        parsed, jerr = safe_json_load(content)
        if parsed:
            st.session_state["last_resp"] = parsed
    else:
        st.info("Nenhum JSON fornecido para montar a tabela.")

# ----- Mostrar resultados estruturados (da √∫ltima resposta / upload) -----
st.markdown("---")
df_table = st.session_state.get("last_df", pd.DataFrame(columns=["nome", "Link de perfil", "Email", "Local e descri√ß√£o"]))
count = int(df_table.shape[0]) if hasattr(df_table, "shape") else 0
st.markdown(f"### Resultados estruturados ‚Äî {count} registros (mostrando at√© {per_page})")

if count == 0:
    st.info("Nenhum registro extra√≠do para a tabela ap√≥s limpeza heur√≠stica.")
else:
    token_to_use = api_token.strip() or os.getenv("THORDATA_TOKEN", "").strip()

    cols = st.columns([3, 2, 2])
    with cols[0]:
        st.write("Op√ß√µes de enriquecimento:")
    with cols[1]:
        force_deep = st.checkbox("For√ßar Enriquecimento (profundo) ‚Äî mais queries/render_js", value=False)
    with cols[2]:
        max_queries_user = st.number_input("Max queries (por candidato)", min_value=1, max_value=100, value=DEFAULT_MAX_QUERIES, step=1)

    if do_extract and token_to_use:
        st.info("Tentando localizar e-mails p√∫blicos para os candidatos (pode consumir requisi√ß√µes/saldo).")
        try:
            df_table = enrich_df_with_emails(df_table, token=token_to_use, engine=engine, domain=domain,
                                             gl=(gl or None), hl=(hl or None), render_js=render_js,
                                             max_per_candidate=int(max_queries_user), force_deep=bool(force_deep))
            st.session_state["last_df"] = df_table
        except Exception as e:
            st.warning(f"Erro ao buscar e-mails: {e}")

    # exibir tabela e bot√µes de a√ß√£o (Cadastrar / Exportar)
    display_df = df_table[["nome", "Link de perfil", "Email", "Local e descri√ß√£o"]].copy()
    display_df["Local e descri√ß√£o"] = display_df["Local e descri√ß√£o"].astype(str).str.replace("\n", " ").str.slice(0, 500)
    st.dataframe(display_df, use_container_width=True)

    btn_col1, btn_col2, btn_col3 = st.columns([1,1,1])
    with btn_col1:
        csv = display_df.to_csv(index=False).encode("utf-8")
        st.download_button("‚¨áÔ∏è Exportar CSV (perfis)", csv, file_name="sourcing_perfis_with_emails.csv", mime="text/csv")
    with btn_col2:
        if st.button("‚úÖ Cadastrar (salvar no DB)"):
            # salvar no DB
            try:
                conn = _conn_for_app
                inserted, ignored = save_profiles_to_db(df_table, conn)
                st.success(f"Registros salvos: {inserted}. Ignorados (duplicados): {ignored}.")
            except Exception as e:
                st.error(f"Erro ao salvar no DB: {e}")
    with btn_col3:
        if st.button("üîÅ Limpar cache de e-mails"):
            if "_email_cache" in st.session_state:
                st.session_state["_email_cache"] = {}
            if CACHE_PATH.exists():
                try:
                    CACHE_PATH.unlink()
                except Exception:
                    pass
            st.success("Cache limpo.")

# mostrar links (clic√°veis)
if count > 0:
    st.markdown("### Links (clique para abrir)")
    for _, r in df_table.head(per_page).iterrows():
        nome = r.get("nome") or "(sem nome)"
        link = r.get("Link de perfil") or ""
        local_desc = (r.get("Local e descri√ß√£o") or "")[:200]
        email = r.get("Email") or ""
        if link:
            if email:
                st.write(f"- [{nome}]({link}) ‚Äî {email} ‚Äî {local_desc}")
            else:
                st.write(f"- [{nome}]({link}) ‚Äî {local_desc}")
        else:
            if email:
                st.write(f"- {nome} ‚Äî {email} ‚Äî {local_desc}")
            else:
                st.write(f"- {nome} ‚Äî {local_desc}")

# painel: visualizar registros j√° cadastrados
st.markdown("---")
st.subheader("üìö Registros cadastrados no banco")
if st.button("Carregar registros do DB"):
    try:
        df_db = fetch_all_profiles(_conn_for_app)
        if df_db.empty:
            st.info("Nenhum registro cadastrado ainda.")
        else:
            st.dataframe(df_db, use_container_width=True)
            csv_db = df_db.to_csv(index=False).encode("utf-8")
            st.download_button("‚¨áÔ∏è Exportar CSV (DB)", csv_db, file_name="sourcing_profiles_db.csv", mime="text/csv")
    except Exception as e:
        st.error(f"Erro ao ler DB: {e}")

# mostrar JSON cru se dispon√≠vel
if show_raw:
    resp_obj = st.session_state.get("last_resp")
    if resp_obj is None:
        st.info("Sem resposta em cache para exibir.")
    else:
        with st.expander("üîß JSON cru (data retornada pela API) ‚Äî expandir para inspecionar"):
            try:
                pretty = json.dumps(resp_obj, ensure_ascii=False, indent=2)
            except Exception:
                pretty = str(resp_obj)
            st.code(pretty[:20000], language="json")

st.markdown("---")
st.markdown(
    "**Aviso de Privacidade e Uso:** Coleta e tratamento de e-mails e nomes tem implica√ß√µes legais (LGPD/GDPR). "
    "Use estes dados apenas sob base legal apropriada, armazene com seguran√ßa e permita remo√ß√£o. "
    "Evite scraping massivo do LinkedIn; prefira ferramentas/licenciamento oficiais quando aplic√°vel."
)
