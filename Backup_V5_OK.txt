# streamlit_thordata_sourcing_no_emails_full.py
# -*- coding: utf-8 -*-
"""
Pesquisa de Perfis - Linkedin ¬∑ Thordata SERP ‚Äî integrado (consulta -> tabela)
Vers√£o SEM busca de e-mail ‚Äî mant√©m persist√™ncia (SQLite), export e consulta.
Export CSV agora pega TODOS os registros do DB; adicionado export XLSX.
"""
import os
import time
import json
import re
import sqlite3
import io
from typing import Any, Dict, List, Optional, Tuple
from pathlib import Path
from datetime import datetime

import requests
import pandas as pd
import streamlit as st

st.set_page_config(page_title="Linkedin - Cogna", layout="wide")

# ---------------- Config / endpoint / DB ----------------
ENDPOINT = "https://scraperapi.thordata.com/request"

# Database file (relative to working dir where streamlit is run)
DB_PATH = Path("./db/sourcing_profiles.db")
DB_PATH.parent.mkdir(parents=True, exist_ok=True)

# ---------------- Settings ----------------
USER_AGENT = "Mozilla/5.0 (compatible; ThordataBot/1.0)"
PER_REQUEST_DELAY = 0.5

# ---------------- Database helpers (no email column) ----------------
def init_db():
    """Cria a tabela se n√£o existir (SEM coluna 'email')."""
    conn = sqlite3.connect(DB_PATH, timeout=30, check_same_thread=False)
    cur = conn.cursor()
    cur.execute("""
    CREATE TABLE IF NOT EXISTS sourcing_profiles (
        id INTEGER PRIMARY KEY AUTOINCREMENT,
        nome TEXT,
        profile_link TEXT,
        local_desc TEXT,
        created_at TEXT
    );
    """)
    cur.execute("CREATE INDEX IF NOT EXISTS idx_profile_link ON sourcing_profiles(profile_link);")
    cur.execute("CREATE INDEX IF NOT EXISTS idx_local_desc ON sourcing_profiles(local_desc);")
    conn.commit()
    return conn

_conn_for_app = init_db()

def save_profiles_to_db(df: pd.DataFrame, conn: sqlite3.Connection) -> Tuple[int, int]:
    """
    Salva registros do DataFrame no DB.
    Evita duplicados verificando profile_link j√° existente.
    Retorna (n_inseridos, n_ignorados).
    """
    if df is None or df.empty:
        return 0, 0

    inserted = 0
    ignored = 0
    cur = conn.cursor()
    now = datetime.utcnow().isoformat()
    with conn:
        for _, r in df.iterrows():
            nome = (r.get("nome") or "").strip()
            link = (r.get("Link de perfil") or "").strip()
            local_desc = (r.get("Local e descri√ß√£o") or "").strip()

            if link:
                cur.execute("SELECT 1 FROM sourcing_profiles WHERE profile_link = ?", (link,))
                if cur.fetchone():
                    ignored += 1
                    continue

            cur.execute(
                "INSERT INTO sourcing_profiles (nome, profile_link, local_desc, created_at) VALUES (?, ?, ?, ?)",
                (nome or None, link or None, local_desc or None, now)
            )
            inserted += 1
    return inserted, ignored

def fetch_all_profiles(conn: sqlite3.Connection) -> pd.DataFrame:
    cur = conn.cursor()
    cur.execute("SELECT id, nome, profile_link, local_desc, created_at FROM sourcing_profiles ORDER BY id DESC")
    rows = cur.fetchall()
    cols = ["id", "nome", "Link de perfil", "Local e descri√ß√£o", "created_at"]
    return pd.DataFrame(rows, columns=cols)

def query_profiles(conn: sqlite3.Connection, location: str = "", competence: str = "") -> pd.DataFrame:
    """
    Consulta o DB por localiza√ß√£o e compet√™ncia (LIKE sobre local_desc).
    """
    cur = conn.cursor()
    sql = "SELECT id, nome, profile_link, local_desc, created_at FROM sourcing_profiles WHERE 1=1"
    params: List[str] = []
    if location and location.strip():
        sql += " AND lower(local_desc) LIKE ?"
        params.append(f"%{location.strip().lower()}%")
    if competence and competence.strip():
        sql += " AND lower(local_desc) LIKE ?"
        params.append(f"%{competence.strip().lower()}%")
    sql += " ORDER BY id DESC"
    cur.execute(sql, params)
    rows = cur.fetchall()
    cols = ["id", "nome", "Link de perfil", "Local e descri√ß√£o", "created_at"]
    return pd.DataFrame(rows, columns=cols)

# ---------------- Utils / Heur√≠sticas ----------------
def extract_name_from_linkedin_title(title: Optional[str]) -> Optional[str]:
    if not title or not isinstance(title, str):
        return None
    s = title.split("|")[0]
    s = s.split(" - ")[0].split(" ‚Äî ")[0].strip()
    return s if s else None

def safe_json_load(obj: Any) -> Tuple[Optional[Dict], Optional[str]]:
    if isinstance(obj, dict):
        return obj, None
    if isinstance(obj, str):
        try:
            parsed = json.loads(obj)
            if isinstance(parsed, dict):
                return parsed, None
            return None, "JSON carregado n√£o √© um objeto dict (esperado)."
        except Exception as e:
            return None, f"Erro ao desserializar string JSON: {e}"
    return None, "Tipo de objeto inesperado; esperava dict ou str contendo JSON."

def find_organic_list(resp: Dict) -> List[Dict]:
    if not isinstance(resp, dict):
        return []
    for key in ("organic", "organic_results", "results", "items"):
        v = resp.get(key)
        if isinstance(v, list):
            return v
    v = resp.get("data")
    if isinstance(v, list):
        return v
    return []

def normalize_item_for_table(item: Dict) -> Dict[str, Optional[str]]:
    title = (item.get("title") or item.get("job_title") or "") if isinstance(item, dict) else ""
    link = item.get("link") or item.get("url") or item.get("source_url") or item.get("final_url") or None
    description = (item.get("description") or item.get("snippet") or "") if isinstance(item, dict) else ""

    name = extract_name_from_linkedin_title(title)
    if not name:
        source = item.get("source") or ""
        if isinstance(source, str) and "LinkedIn" in source:
            parts = re.split(r"¬∑|-", source)
            if parts:
                candidate = parts[-1].strip()
                if candidate and len(candidate) > 1:
                    name = candidate
    if not name:
        m = re.search(r'\b([A-Z√Ä-≈∏][a-z√†-√ø]+(?:\s+[A-Z√Ä-≈∏][a-z√†-√ø]+){0,2})\b', title or "")
        if m:
            name = m.group(1)
    local_desc = description or ""
    return {
        "nome": name or "",
        "Link de perfil": link or "",
        "Local e descri√ß√£o": local_desc or ""
    }

def resp_to_table(resp_obj: Any, max_rows: int = 10) -> Tuple[pd.DataFrame, Optional[str]]:
    parsed, err = safe_json_load(resp_obj)
    if err:
        return pd.DataFrame(columns=["nome", "Link de perfil", "Local e descri√ß√£o"]), err

    organic_list = find_organic_list(parsed)
    rows = []
    for item in organic_list:
        row = normalize_item_for_table(item)
        rows.append(row)

    if not rows:
        return pd.DataFrame(columns=["nome", "Link de perfil", "Local e descri√ß√£o"]), None

    df = pd.DataFrame(rows)
    df = df.head(max_rows).reset_index(drop=True)
    return df, None

def exponential_backoff_sleep(attempt: int):
    wait = min(30, 2 ** attempt)
    time.sleep(wait)

def thordata_search(token: str,
                    q: str,
                    engine: str = "google",
                    domain: Optional[str] = None,
                    gl: Optional[str] = None,
                    hl: Optional[str] = None,
                    start: Optional[int] = None,
                    num: Optional[int] = None,
                    render_js: bool = False,
                    extra_params: Optional[Dict[str, Any]] = None,
                    max_retries: int = 6) -> Any:
    """
    Executa a chamada POST para Thordata/TheirData.
    Retorna dict (quando JSON) ou string (quando resp.text).
    """
    if not token:
        raise RuntimeError("Token n√£o informado. Defina THORDATA_TOKEN no ambiente ou cole na UI.")
    headers = {
        "Authorization": f"Bearer {token}",
        "Content-Type": "application/x-www-form-urlencoded"
    }
    data = {"engine": engine, "q": q, "json": "1"}
    if domain: data["domain"] = domain
    if gl: data["gl"] = gl
    if hl: data["hl"] = hl
    if start is not None: data["start"] = str(start)
    if num is not None: data["num"] = str(num)
    if render_js: data["render_js"] = "1"
    if extra_params:
        for k, v in (extra_params.items() if isinstance(extra_params, dict) else []):
            if v is not None:
                data[k] = str(v)

    attempt = 0
    while True:
        resp = requests.post(ENDPOINT, headers=headers, data=data, timeout=60)
        if resp.status_code == 200:
            try:
                return resp.json()
            except ValueError:
                return resp.text
        if resp.status_code == 429:
            if attempt >= max_retries:
                raise RuntimeError("Rate limited: excedeu tentativas (429).")
            exponential_backoff_sleep(attempt)
            attempt += 1
            continue
        if resp.status_code == 401:
            raise RuntimeError("401 Unauthorized - token inv√°lido ou expirado.")
        if resp.status_code == 402:
            raise RuntimeError("402 Payment Required - saldo insuficiente.")
        resp.raise_for_status()

# ---------------- UI ----------------
st.title("üîé   Pesquisa de Perfis - Linkedin ")
st.caption(f"DB: {DB_PATH.resolve()}")
st.markdown("Execute a busca; o resultado ser√° automaticamente estruturado em tabela (nome, Link de perfil, Local e descri√ß√£o).")

with st.sidebar:
    st.header("Configura√ß√µes API / Query")
    env_token = os.getenv("THORDATA_TOKEN", "")
    api_token = st.text_input("TheirData API Key (Bearer)", value=env_token, type="password",
                              help="Recomendado: defina THORDATA_TOKEN no ambiente.")
    st.markdown("---")
    st.header("Par√¢metros padr√£o")
    engine = st.selectbox("Mecanismo", options=["google", "bing"], index=0, key="mecanismo")
    domain = st.selectbox("Dom√≠nio Google", options=["google.com", "google.com.br", "google.co.uk"], index=0, key="domain_select")
    gl = st.selectbox("Pa√≠s (gl)", options=["BR", "US", "CA", "UK", ""], index=0, key="gl_select")
    hl = st.selectbox("Idioma (hl)", options=["pt-BR", "en", "pt", ""], index=0, key="hl_select")
    render_js = st.checkbox("Renderizar JS (mais lento/custoso)", value=False, key="render_js_sidebar")

with st.form("search_form"):
    st.subheader("Filtros de busca")
    area = st.selectbox("√Årea (ex.:)", ["Data Science", "Software Engineering", "DevOps", "Security", "Product", "Design", "Sales/Marketing", "Outro"], index=0, key="area_sel")
    competence = st.text_input("Compet√™ncia / skill (ex.: Python, AWS, Spark)", placeholder="python, aws, spark", key="competence_input")
    location = st.text_input("Localidade (cidade / estado / pa√≠s)", placeholder="S√£o Paulo, Brazil", key="location_input")
    free_text = st.text_input("Termos adicionais (ex.: 'Bacharel', 'Mestrado', 'S√™nior')", placeholder="", key="free_text_input")
    linkedin_only = st.checkbox("Somente LinkedIn (perfils) ‚Äî site:linkedin.com/in", value=False,
                                help="Se marcado, a query ser√° prefixada com site:linkedin.com/in OR site:linkedin.com/pub",
                                key="linkedin_only_cb")
    per_page = st.slider("Resultados por p√°gina (limite para tabela)", min_value=5, max_value=50, value=10, step=5, key="per_page_slider")
    page_idx = st.number_input("P√°gina (0 = primeira)", min_value=0, value=0, step=1, key="page_idx_num")
    show_raw = st.checkbox("Mostrar JSON cru (ap√≥s consulta)", value=False, key="show_raw_cb")
    submitted = st.form_submit_button("üîé Pesquisar", key="form_submit_btn")

if "last_resp" not in st.session_state:
    st.session_state["last_resp"] = None
if "last_df" not in st.session_state:
    st.session_state["last_df"] = pd.DataFrame(columns=["nome", "Link de perfil", "Local e descri√ß√£o"])
if "consulta_open" not in st.session_state:
    st.session_state["consulta_open"] = False

def build_query(area: str, competence: str, location: str, free_text: str, linkedin_only: bool) -> str:
    parts = []
    if area and area != "Outro":
        parts.append(area)
    if competence:
        parts.append(competence)
    if location:
        parts.append(location)
    if free_text:
        parts.append(free_text)
    q = " ".join(parts).strip()
    if linkedin_only:
        if q:
            q = f"(site:linkedin.com/in OR site:linkedin.com/pub) {q}"
        else:
            q = "(site:linkedin.com/in OR site:linkedin.com/pub)"
    return q

# Quando o usu√°rio submete a busca
if submitted:
    token_to_use = api_token.strip() or os.getenv("THORDATA_TOKEN", "").strip()
    if not token_to_use:
        st.error("Token n√£o fornecido. Defina THORDATA_TOKEN no ambiente ou cole a chave no campo da lateral.")
    else:
        q = build_query(area, competence, location, free_text, linkedin_only)
        if not q:
            st.warning("Query vazia ‚Äî informe ao menos uma compet√™ncia, √°rea ou localidade.")
        else:
            start = page_idx * per_page
            with st.spinner("Consultando Thordata (SERP)..."):
                try:
                    resp_obj = thordata_search(token=token_to_use, q=q, engine=engine,
                                              domain=domain, gl=(gl or None), hl=(hl or None),
                                              start=start, num=per_page, render_js=render_js)
                except Exception as e:
                    st.error(f"Erro na busca: {e}")
                    resp_obj = None

            st.session_state["last_resp"] = resp_obj

            if resp_obj is not None:
                df_table, err = resp_to_table(resp_obj, max_rows=per_page)
                if err:
                    st.warning(err)
                st.session_state["last_df"] = df_table
            else:
                st.session_state["last_df"] = pd.DataFrame(columns=["nome", "Link de perfil", "Local e descri√ß√£o"])

# area para colar JSON ou fazer upload (montar tabela sem consultar API)
st.markdown("---")
st.subheader("Ou: cole / carregue um JSON retornado pela API (opcional)")
col1, col2 = st.columns([3, 1])
with col1:
    pasted = st.text_area("Cole o JSON aqui (opcional)", height=140, placeholder='Cole aqui o JSON retornado pela API...', key="pasted_json")
with col2:
    upload = st.file_uploader("Ou fa√ßa upload do arquivo JSON", type=["json"], key="upload_json")

# bot√£o com key √∫nico para montar tabela a partir do JSON
if st.button("üîß Montar tabela a partir do JSON colado/subido", key="montar_json_btn"):
    content = None
    if upload is not None:
        try:
            raw = upload.read()
            content = raw.decode("utf-8")
        except Exception as e:
            st.error(f"Erro lendo arquivo: {e}")
    elif pasted and pasted.strip():
        content = pasted.strip()

    if content:
        df_table, err = resp_to_table(content, max_rows=per_page)
        if err:
            st.warning(err)
        st.session_state["last_df"] = df_table
        parsed, jerr = safe_json_load(content)
        if parsed:
            st.session_state["last_resp"] = parsed
    else:
        st.info("Nenhum JSON fornecido para montar a tabela.")

# ----- Mostrar resultados estruturados (da √∫ltima resposta / upload) -----
st.markdown("---")
df_table = st.session_state.get("last_df", pd.DataFrame(columns=["nome", "Link de perfil", "Local e descri√ß√£o"]))
count = int(df_table.shape[0]) if hasattr(df_table, "shape") else 0
st.markdown(f"### Resultados estruturados ‚Äî {count} registros (mostrando at√© {per_page})")

if count == 0:
    st.info("Nenhum registro extra√≠do para a tabela ap√≥s limpeza heur√≠stica.")
else:
    display_df = df_table[["nome", "Link de perfil", "Local e descri√ß√£o"]].copy()
    display_df["Local e descri√ß√£o"] = display_df["Local e descri√ß√£o"].astype(str).str.replace("\n", " ").str.slice(0, 500)
    st.dataframe(display_df, use_container_width=True)

    # === EXPORT: agora exporta TODO o DB (CSV) e tamb√©m XLSX ===
    export_col1, export_col2, export_col3 = st.columns([1,1,1])

    # Export ALL DB as CSV
    with export_col1:
        if st.button("‚¨áÔ∏è Exportar CSV (DB ‚Äî todos perfis)", key="export_db_csv_btn"):
            try:
                df_db = fetch_all_profiles(_conn_for_app)
                if df_db.empty:
                    st.info("Banco vazio ‚Äî nada para exportar.")
                else:
                    csv_bytes = df_db.to_csv(index=False).encode("utf-8")
                    st.download_button(
                        "Clique para baixar CSV (DB)",
                        data=csv_bytes,
                        file_name="sourcing_profiles_db.csv",
                        mime="text/csv",
                        key="download_db_csv_btn"
                    )
            except Exception as e:
                st.error(f"Falha ao exportar CSV do DB: {e}")

    # Export ALL DB as XLSX
    with export_col2:
        if st.button("‚¨áÔ∏è Exportar XLSX (DB ‚Äî todos perfis)", key="export_db_xlsx_btn"):
            try:
                df_db = fetch_all_profiles(_conn_for_app)
                if df_db.empty:
                    st.info("Banco vazio ‚Äî nada para exportar.")
                else:
                    out = io.BytesIO()
                    # use openpyxl engine (pandas)
                    with pd.ExcelWriter(out, engine="openpyxl") as xw:
                        # escrever com sheet name "Perfis"
                        df_db.to_excel(xw, sheet_name="Perfis", index=False)
                        # ajustar ponteiro
                    out.seek(0)
                    st.download_button(
                        "Clique para baixar XLSX (DB)",
                        data=out.getvalue(),
                        file_name="sourcing_profiles_db.xlsx",
                        mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                        key="download_db_xlsx_btn"
                    )
            except Exception as e:
                st.error(f"Falha ao exportar XLSX do DB: {e}\n(Verifique se openpyxl est√° instalado: pip install openpyxl)")

    # Keep a button to export only the shown profiles (optional)
    with export_col3:
        csv_shown = display_df.to_csv(index=False).encode("utf-8")
        st.download_button("‚¨áÔ∏è Exportar CSV (tabela exibida)", csv_shown, file_name="sourcing_perfis_displayed.csv", mime="text/csv", key="download_displayed_csv_btn")

    # bot√µes salvar e reset (com keys √∫nicas)
    btn_col1, btn_col2 = st.columns([1,1])
    with btn_col1:
        if st.button("‚úÖ Cadastrar (salvar no DB)", key="cadastrar_db_btn"):
            try:
                conn = _conn_for_app
                inserted, ignored = save_profiles_to_db(df_table, conn)
                st.success(f"Registros salvos: {inserted}. Ignorados (duplicados): {ignored}.")
            except Exception as e:
                st.error(f"Erro ao salvar no DB: {e}")
    with btn_col2:
        if st.button("üîÅ Reset sess√£o (remove last_df)", key="reset_last_df_btn"):
            st.session_state["last_df"] = pd.DataFrame(columns=["nome", "Link de perfil", "Local e descri√ß√£o"])
            st.success("Sess√£o reiniciada (last_df limpo).")

# mostrar links (clic√°veis)
if count > 0:
    st.markdown("### Links (clique para abrir)")
    for _, r in df_table.head(per_page).iterrows():
        nome = r.get("nome") or "(sem nome)"
        link = r.get("Link de perfil") or ""
        local_desc = (r.get("Local e descri√ß√£o") or "")[:200]
        if link:
            st.write(f"- [{nome}]({link}) ‚Äî {local_desc}")
        else:
            st.write(f"- {nome} ‚Äî {local_desc}")

# painel: visualizar registros j√° cadastrados no banco (com bot√£o Consulta)
st.markdown("---")
st.subheader("üìö Registros cadastrados no banco")
btns = st.columns([1,1,1])
with btns[0]:
    if st.button("Carregar registros do DB", key="carregar_db_btn"):
        try:
            df_db = fetch_all_profiles(_conn_for_app)
            if df_db.empty:
                st.info("Nenhum registro cadastrado ainda.")
            else:
                st.dataframe(df_db, use_container_width=True)
                csv_db = df_db.to_csv(index=False).encode("utf-8")
                st.download_button("‚¨áÔ∏è Exportar CSV (DB)", csv_db, file_name="sourcing_profiles_db.csv", mime="text/csv", key="download_db_btn_panel")
        except Exception as e:
            st.error(f"Erro ao ler DB: {e}")
with btns[1]:
    # bot√£o Consulta: abre a "outra tela" para consulta por Localiza√ß√£o e Compet√™ncia
    if st.button("üîé Consulta", key="open_consulta_btn"):
        st.session_state["consulta_open"] = True
with btns[2]:
    if st.button("üîÅ Reset sess√£o (remove last_df)", key="reset_session_btn"):
        st.session_state["last_df"] = pd.DataFrame(columns=["nome", "Link de perfil", "Local e descri√ß√£o"])
        st.success("Sess√£o reiniciada (last_df limpo).")

# Consulta: tela/modal (renderiza√ß√£o condicional)
if st.session_state.get("consulta_open", False):
    st.markdown("---")
    st.header("üîç Consulta no banco ‚Äî Filtrar por Localiza√ß√£o e Compet√™ncia")
    st.markdown("Preencha um ou ambos os campos. A busca far√° LIKE (case-insensitive) sobre o campo `Local e descri√ß√£o`.")
    col_a, col_b = st.columns(2)
    with col_a:
        consulta_location = st.text_input("Localiza√ß√£o (ex.: S√£o Paulo, Campinas, Brasil)", value="", key="consulta_location_input")
    with col_b:
        consulta_competence = st.text_input("Compet√™ncia (ex.: Python, AWS, DevOps)", value="", key="consulta_competence_input")

    consulta_cols = st.columns([1,1,1])
    with consulta_cols[0]:
        if st.button("üîé Buscar", key="consulta_buscar_btn"):
            try:
                df_res = query_profiles(_conn_for_app, location=consulta_location, competence=consulta_competence)
                if df_res.empty:
                    st.info("Nenhum registro encontrado para os crit√©rios fornecidos.")
                else:
                    st.success(f"Encontrados {len(df_res)} registros.")
                    st.dataframe(df_res, use_container_width=True)
                    csv_r = df_res.to_csv(index=False).encode("utf-8")
                    st.download_button("‚¨áÔ∏è Exportar CSV (consulta)", csv_r, file_name="consulta_sourcing_profiles.csv", mime="text/csv", key="download_consulta_btn")
            except Exception as e:
                st.error(f"Erro na consulta: {e}")
    with consulta_cols[1]:
        if st.button("üßæ Mostrar todos", key="consulta_mostrar_todos_btn"):
            try:
                df_all = fetch_all_profiles(_conn_for_app)
                if df_all.empty:
                    st.info("Nenhum registro cadastrado.")
                else:
                    st.dataframe(df_all, use_container_width=True)
                    csv_all = df_all.to_csv(index=False).encode("utf-8")
                    st.download_button("‚¨áÔ∏è Exportar CSV (todos)", csv_all, file_name="all_sourcing_profiles.csv", mime="text/csv", key="download_all_btn")
            except Exception as e:
                st.error(f"Erro ao carregar todos: {e}")
    with consulta_cols[2]:
        if st.button("‚úñ Fechar / Voltar", key="consulta_fechar_btn"):
            st.session_state["consulta_open"] = False
            st.experimental_rerun()

# mostrar JSON cru se dispon√≠vel
if show_raw:
    resp_obj = st.session_state.get("last_resp")
    if resp_obj is None:
        st.info("Sem resposta em cache para exibir.")
    else:
        with st.expander("üîß JSON cru (data retornada pela API) ‚Äî expandir para inspecionar"):
            try:
                pretty = json.dumps(resp_obj, ensure_ascii=False, indent=2)
            except Exception:
                pretty = str(resp_obj)
            st.code(pretty[:20000], language="json")

st.markdown("---")
st.markdown(
    "**Aviso de Privacidade e Uso:** Mesmo sem busca por e-mails, trate nomes e links com responsabilidade (LGPD/GDPR)."
)
