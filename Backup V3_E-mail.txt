# streamlit_thordata_sourcing_with_emails.py
# -*- coding: utf-8 -*-
"""
Pesquisa de Perfis - Linkedin Â· Thordata SERP â€” integrado (consulta -> tabela -> tentativa de e-mail)
ModificaÃ§Ãµes:
 - para cada registro extraÃ­do, tenta uma segunda busca para localizar e-mails pÃºblicos
 - insere coluna "Email" entre "Link de perfil" e "Local e descriÃ§Ã£o"
 - atenÃ§Ã£o: resultados nÃ£o garantidos; veja aviso de privacidade no fim
"""
import os
import time
import json
import re
from typing import Any, Dict, List, Optional, Tuple

import requests
import pandas as pd
import streamlit as st

st.set_page_config(page_title="Pesquisa de Perfis - Linkedin", layout="wide")

# ---------------- Config / endpoint ----------------
ENDPOINT = "https://scraperapi.thordata.com/request"

# ---------------- Utils / HeurÃ­sticas ----------------
EMAIL_RE = re.compile(r'[\w\.-]+@[\w\.-]+\.\w+', re.I)

def extract_name_from_linkedin_title(title: Optional[str]) -> Optional[str]:
    if not title or not isinstance(title, str):
        return None
    s = title.split("|")[0]
    s = s.split(" - ")[0].split(" â€” ")[0].strip()
    return s if s else None

def safe_json_load(obj: Any) -> Tuple[Optional[Dict], Optional[str]]:
    if isinstance(obj, dict):
        return obj, None
    if isinstance(obj, str):
        try:
            parsed = json.loads(obj)
            if isinstance(parsed, dict):
                return parsed, None
            return None, "JSON carregado nÃ£o Ã© um objeto dict (esperado)."
        except Exception as e:
            return None, f"Erro ao desserializar string JSON: {e}"
    return None, "Tipo de objeto inesperado; esperava dict ou str contendo JSON."

def find_organic_list(resp: Dict) -> List[Dict]:
    if not isinstance(resp, dict):
        return []
    for key in ("organic", "organic_results", "results", "items"):
        v = resp.get(key)
        if isinstance(v, list):
            return v
    v = resp.get("data")
    if isinstance(v, list):
        return v
    return []

def normalize_item_for_table(item: Dict) -> Dict[str, Optional[str]]:
    title = (item.get("title") or item.get("job_title") or "") if isinstance(item, dict) else ""
    link = item.get("link") or item.get("url") or item.get("source_url") or item.get("final_url") or None
    description = (item.get("description") or item.get("snippet") or "") if isinstance(item, dict) else ""

    name = extract_name_from_linkedin_title(title)
    if not name:
        source = item.get("source") or ""
        if isinstance(source, str) and "LinkedIn" in source:
            parts = re.split(r"Â·|-", source)
            if parts:
                candidate = parts[-1].strip()
                if candidate and len(candidate) > 1:
                    name = candidate
    if not name:
        m = re.search(r'\b([A-ZÃ€-Å¸][a-zÃ -Ã¿]+(?:\s+[A-ZÃ€-Å¸][a-zÃ -Ã¿]+){0,2})\b', title or "")
        if m:
            name = m.group(1)
    local_desc = description or ""
    return {
        "nome": name or "",
        "Link de perfil": link or "",
        # placeholder for Email (serÃ¡ preenchido depois)
        "Email": "",
        "Local e descriÃ§Ã£o": local_desc or ""
    }

def resp_to_table(resp_obj: Any, max_rows: int = 10) -> Tuple[pd.DataFrame, Optional[str]]:
    parsed, err = safe_json_load(resp_obj)
    if err:
        return pd.DataFrame(columns=["nome", "Link de perfil", "Email", "Local e descriÃ§Ã£o"]), err

    organic_list = find_organic_list(parsed)
    rows = []
    for item in organic_list:
        row = normalize_item_for_table(item)
        rows.append(row)

    if not rows:
        return pd.DataFrame(columns=["nome", "Link de perfil", "Email", "Local e descriÃ§Ã£o"]), None

    df = pd.DataFrame(rows)
    df = df.head(max_rows).reset_index(drop=True)
    return df, None

# ---------------- HTTP + Backoff (Thordata) ----------------
def exponential_backoff_sleep(attempt: int):
    wait = min(30, 2 ** attempt)
    time.sleep(wait)

def thordata_search(token: str,
                    q: str,
                    engine: str = "google",
                    domain: Optional[str] = None,
                    gl: Optional[str] = None,
                    hl: Optional[str] = None,
                    start: Optional[int] = None,
                    num: Optional[int] = None,
                    render_js: bool = False,
                    extra_params: Optional[Dict[str, Any]] = None,
                    max_retries: int = 6) -> Any:
    if not token:
        raise RuntimeError("Token nÃ£o informado. Defina THORDATA_TOKEN no ambiente ou cole na UI.")
    headers = {
        "Authorization": f"Bearer {token}",
        "Content-Type": "application/x-www-form-urlencoded"
    }
    data = {"engine": engine, "q": q, "json": "1"}
    if domain: data["domain"] = domain
    if gl: data["gl"] = gl
    if hl: data["hl"] = hl
    if start is not None: data["start"] = str(start)
    if num is not None: data["num"] = str(num)
    if render_js: data["render_js"] = "1"
    if extra_params:
        for k, v in (extra_params.items() if isinstance(extra_params, dict) else []):
            if v is not None:
                data[k] = str(v)

    attempt = 0
    while True:
        resp = requests.post(ENDPOINT, headers=headers, data=data, timeout=60)
        if resp.status_code == 200:
            try:
                return resp.json()
            except ValueError:
                return resp.text
        if resp.status_code == 429:
            if attempt >= max_retries:
                raise RuntimeError("Rate limited: excedeu tentativas (429).")
            exponential_backoff_sleep(attempt)
            attempt += 1
            continue
        if resp.status_code == 401:
            raise RuntimeError("401 Unauthorized - token invÃ¡lido ou expirado.")
        if resp.status_code == 402:
            raise RuntimeError("402 Payment Required - saldo insuficiente.")
        resp.raise_for_status()

# ---------------- Email search helpers ----------------
def extract_emails_from_obj(obj: Any) -> List[str]:
    """Procura por padrÃµes de e-mail em um objeto (dict/list/str) retornado pela Thordata."""
    found = set()
    try:
        if isinstance(obj, str):
            for m in EMAIL_RE.findall(obj):
                found.add(m.lower())
            return list(found)
        if isinstance(obj, dict):
            # inspect obvious text fields
            for key in ("snippet", "description", "title", "content", "text"):
                v = obj.get(key)
                if isinstance(v, str):
                    for m in EMAIL_RE.findall(v):
                        found.add(m.lower())
            # also search within any organic list
            for k, v in obj.items():
                if isinstance(v, str):
                    for m in EMAIL_RE.findall(v):
                        found.add(m.lower())
                elif isinstance(v, list):
                    for item in v:
                        if isinstance(item, str):
                            for m in EMAIL_RE.findall(item):
                                found.add(m.lower())
                        elif isinstance(item, dict):
                            for kk in ("snippet","description","title","url","link"):
                                vv = item.get(kk)
                                if isinstance(vv, str):
                                    for m in EMAIL_RE.findall(vv):
                                        found.add(m.lower())
    except Exception:
        pass
    return list(found)

def build_email_search_query(name: str, location: str = "") -> str:
    """
    Monta uma query para tentar encontrar e-mails pÃºblicos relacionados ao nome.
    EstratÃ©gia simples: procurar por 'Nome' + localidade + termos 'email' OR '@' OR 'contato'
    """
    qparts = []
    if name:
        # entre aspas para priorizar combinaÃ§Ã£o exata
        qparts.append(f'"{name}"')
    if location:
        qparts.append(f'"{location}"')
    qparts.append('(email OR contato OR "@" OR "e-mail")')
    # evitar filtrar apenas linkedin (queremos pÃ¡ginas pessoais)
    return " ".join(qparts)

def find_email_for_name(token: str, name: str, location: str = "", engine: str = "google",
                        domain: Optional[str] = None, gl: Optional[str] = None, hl: Optional[str] = None,
                        max_pages: int = 1, render_js: bool = False) -> Optional[str]:
    """
    Tenta localizar um e-mail pÃºblico para 'name' atravÃ©s de buscas SERP via Thordata.
    Retorna o primeiro e-mail plausÃ­vel encontrado (string) ou None.
    """
    if not name:
        return None
    q = build_email_search_query(name, location)
    try:
        resp = thordata_search(token=token, q=q, engine=engine, domain=domain, gl=gl, hl=hl,
                               start=0, num=10, render_js=render_js)
    except Exception as e:
        # falha na busca -> nÃ£o encontramos
        return None

    # extrair em vÃ¡rios pontos do JSON / snippets / organic
    emails = set()
    if isinstance(resp, dict):
        # scan top-level fields
        for v in resp.values():
            emails.update([e for e in extract_emails_from_obj(v)])
        # and organic entries
        organics = find_organic_list(resp)
        for item in organics:
            # search snippet, title, link fields
            for field in ("snippet", "description", "title", "link", "url"):
                vv = item.get(field)
                if isinstance(vv, str):
                    for m in EMAIL_RE.findall(vv):
                        emails.add(m.lower())
            # sometimes snippet contains html/text with e-mails
            emails.update([e for e in extract_emails_from_obj(item)])
    elif isinstance(resp, str):
        emails.update([m.lower() for m in EMAIL_RE.findall(resp)])

    # heurÃ­stica: prefer corporate emails over generic ones? (not implemented here)
    if emails:
        # return first unique email
        return sorted(emails)[0]
    return None

def enrich_df_with_emails(df: pd.DataFrame, token: str, engine: str, domain: Optional[str],
                          gl: Optional[str], hl: Optional[str], render_js: bool, max_per_candidate: int = 1) -> pd.DataFrame:
    """
    Para cada linha do DataFrame, tenta encontrar e preencher a coluna 'Email'.
    Retorna novo DataFrame com a coluna atualizada.
    """
    if df is None or df.empty:
        return df
    # ensure 'Email' column exists
    if "Email" not in df.columns:
        df["Email"] = ""

    total = df.shape[0]
    progress = st.progress(0)
    for i, row in df.iterrows():
        # se jÃ¡ preenchido, pula
        if row.get("Email"):
            progress.progress(int((i+1)/total*100))
            continue
        name = (row.get("nome") or "").strip()
        # tentar localizar contato pela descriÃ§Ã£o/local tambÃ©m
        local = (row.get("Local e descriÃ§Ã£o") or "")
        try:
            email = find_email_for_name(token=token, name=name, location=local, engine=engine,
                                        domain=domain, gl=gl, hl=hl, render_js=render_js)
        except Exception:
            email = None
        if email:
            df.at[i, "Email"] = email
        else:
            df.at[i, "Email"] = ""  # keep empty or mark "nÃ£o encontrado"
        # small delay to avoid rate flooding (tune as needed)
        time.sleep(1.0)
        progress.progress(int((i+1)/total*100))
    progress.empty()
    return df

# ---------------- UI ----------------
st.title("ðŸ”Ž   Pesquisa de Perfis - Linkedin")
st.markdown("Execute a busca; o resultado serÃ¡ automaticamente estruturado em tabela (nome, Link de perfil, Email, Local e descriÃ§Ã£o).")

with st.sidebar:
    st.header("ConfiguraÃ§Ãµes API / Query")
    env_token = os.getenv("THORDATA_TOKEN", "")
    api_token = st.text_input("TheirData API Key (Bearer)", value=env_token, type="password",
                              help="Recomendado: defina THORDATA_TOKEN no ambiente.")
    st.markdown("---")
    st.header("ParÃ¢metros padrÃ£o")
    engine = st.selectbox("Mecanismo", options=["google", "bing"], index=0)
    domain = st.selectbox("DomÃ­nio Google", options=["google.com", "google.com.br", "google.co.uk"], index=0)
    gl = st.selectbox("PaÃ­s (gl)", options=["BR", "US", "CA", "UK", ""], index=0)
    hl = st.selectbox("Idioma (hl)", options=["pt-BR", "en", "pt", ""], index=0)
    render_js = st.checkbox("Renderizar JS (mais lento/custoso)", value=False)
    st.markdown("---")
    do_extract = st.checkbox("Extrair nomes e e-mails automaticamente", value=True, key="do_extract_sidebar")

with st.form("search_form"):
    st.subheader("Filtros de busca")
    area = st.selectbox("Ãrea (ex.:)", ["Data Science", "Software Engineering", "DevOps", "Security", "Product", "Design", "Sales/Marketing", "Outro"], index=0)
    competence = st.text_input("CompetÃªncia / skill (ex.: Python, AWS, Spark)", placeholder="python, aws, spark")
    location = st.text_input("Localidade (cidade / estado / paÃ­s)", placeholder="SÃ£o Paulo, Brazil")
    free_text = st.text_input("Termos adicionais (ex.: 'Bacharel', 'Mestrado', 'SÃªnior')", placeholder="")
    linkedin_only = st.checkbox("Somente LinkedIn (perfils) â€” site:linkedin.com/in", value=False,
                                help="Se marcado, a query serÃ¡ prefixada com site:linkedin.com/in OR site:linkedin.com/pub")
    per_page = st.slider("Resultados por pÃ¡gina (limite para tabela)", min_value=5, max_value=50, value=10, step=5)
    page_idx = st.number_input("PÃ¡gina (0 = primeira)", min_value=0, value=0, step=1)
    show_raw = st.checkbox("Mostrar JSON cru (apÃ³s consulta)", value=False)
    submitted = st.form_submit_button("ðŸ”Ž Pesquisar")

if "last_resp" not in st.session_state:
    st.session_state["last_resp"] = None
if "last_df" not in st.session_state:
    st.session_state["last_df"] = pd.DataFrame(columns=["nome", "Link de perfil", "Email", "Local e descriÃ§Ã£o"])

def build_query(area: str, competence: str, location: str, free_text: str, linkedin_only: bool) -> str:
    parts = []
    if area and area != "Outro":
        parts.append(area)
    if competence:
        parts.append(competence)
    if location:
        parts.append(location)
    if free_text:
        parts.append(free_text)
    q = " ".join(parts).strip()
    if linkedin_only:
        if q:
            q = f"(site:linkedin.com/in OR site:linkedin.com/pub) {q}"
        else:
            q = "(site:linkedin.com/in OR site:linkedin.com/pub)"
    return q

if submitted:
    token_to_use = api_token.strip() or os.getenv("THORDATA_TOKEN", "").strip()
    if not token_to_use:
        st.error("Token nÃ£o fornecido. Defina THORDATA_TOKEN no ambiente ou cole a chave no campo da lateral.")
    else:
        q = build_query(area, competence, location, free_text, linkedin_only)
        if not q:
            st.warning("Query vazia â€” informe ao menos uma competÃªncia, Ã¡rea ou localidade.")
        else:
            start = page_idx * per_page
            with st.spinner("Consultando Thordata (SERP)..."):
                try:
                    resp_obj = thordata_search(token=token_to_use, q=q, engine=engine,
                                              domain=domain, gl=(gl or None), hl=(hl or None),
                                              start=start, num=per_page, render_js=render_js)
                except Exception as e:
                    st.error(f"Erro na busca: {e}")
                    resp_obj = None

            st.session_state["last_resp"] = resp_obj

            if resp_obj is not None:
                df_table, err = resp_to_table(resp_obj, max_rows=per_page)
                if err:
                    st.warning(err)
                st.session_state["last_df"] = df_table
            else:
                st.session_state["last_df"] = pd.DataFrame(columns=["nome", "Link de perfil", "Email", "Local e descriÃ§Ã£o"])

# area para colar JSON ou fazer upload (montar tabela sem consultar API)
st.markdown("---")
st.subheader("Ou: cole / carregue um JSON retornado pela API (opcional)")
col1, col2 = st.columns([3, 1])
with col1:
    pasted = st.text_area("Cole o JSON aqui (opcional)", height=140, placeholder='Cole aqui o JSON retornado pela API...')
with col2:
    upload = st.file_uploader("Ou faÃ§a upload do arquivo JSON", type=["json"])

if st.button("ðŸ”§ Montar tabela a partir do JSON colado/subido"):
    content = None
    if upload is not None:
        try:
            raw = upload.read()
            content = raw.decode("utf-8")
        except Exception as e:
            st.error(f"Erro lendo arquivo: {e}")
    elif pasted and pasted.strip():
        content = pasted.strip()

    if content:
        df_table, err = resp_to_table(content, max_rows=per_page)
        if err:
            st.warning(err)
        st.session_state["last_df"] = df_table
        parsed, jerr = safe_json_load(content)
        if parsed:
            st.session_state["last_resp"] = parsed
    else:
        st.info("Nenhum JSON fornecido para montar a tabela.")

# ----- Mostrar resultados estruturados (da Ãºltima resposta / upload) -----
st.markdown("---")
df_table = st.session_state.get("last_df", pd.DataFrame(columns=["nome", "Link de perfil", "Email", "Local e descriÃ§Ã£o"]))
count = int(df_table.shape[0]) if hasattr(df_table, "shape") else 0
st.markdown(f"### Resultados estruturados â€” {count} registros (mostrando atÃ© {per_page})")

if count == 0:
    st.info("Nenhum registro extraÃ­do para a tabela apÃ³s limpeza heurÃ­stica.")
else:
    # se usuÃ¡rio pediu extraÃ§Ã£o automÃ¡tica de e-mails, tentamos enriquecer
    token_to_use = api_token.strip() or os.getenv("THORDATA_TOKEN", "").strip()
    if do_extract and token_to_use:
        st.info("Tentando localizar e-mails pÃºblicos para os candidatos (pode consumir requisiÃ§Ãµes/saldo).")
        try:
            df_table = enrich_df_with_emails(df_table, token=token_to_use, engine=engine, domain=domain,
                                             gl=(gl or None), hl=(hl or None), render_js=render_js)
            st.session_state["last_df"] = df_table
        except Exception as e:
            st.warning(f"Erro ao buscar e-mails: {e}")

    # exibir tabela com colunas na ordem desejada (nome, link, email, local/descriÃ§Ã£o)
    display_df = df_table[["nome", "Link de perfil", "Email", "Local e descriÃ§Ã£o"]].copy()
    display_df["Local e descriÃ§Ã£o"] = display_df["Local e descriÃ§Ã£o"].astype(str).str.replace("\n", " ").str.slice(0, 500)
    st.dataframe(display_df, use_container_width=True)
    csv = display_df.to_csv(index=False).encode("utf-8")
    st.download_button("â¬‡ï¸ Exportar CSV (perfis)", csv, file_name="sourcing_perfis_with_emails.csv", mime="text/csv")

# mostrar links (clicÃ¡veis)
if count > 0:
    st.markdown("### Links (clique para abrir)")
    for _, r in df_table.head(per_page).iterrows():
        nome = r.get("nome") or "(sem nome)"
        link = r.get("Link de perfil") or ""
        local_desc = (r.get("Local e descriÃ§Ã£o") or "")[:200]
        email = r.get("Email") or ""
        if link:
            if email:
                st.write(f"- [{nome}]({link}) â€” {email} â€” {local_desc}")
            else:
                st.write(f"- [{nome}]({link}) â€” {local_desc}")
        else:
            if email:
                st.write(f"- {nome} â€” {email} â€” {local_desc}")
            else:
                st.write(f"- {nome} â€” {local_desc}")

# mostrar JSON cru se disponÃ­vel
if show_raw:
    resp_obj = st.session_state.get("last_resp")
    if resp_obj is None:
        st.info("Sem resposta em cache para exibir.")
    else:
        with st.expander("ðŸ”§ JSON cru (data retornada pela API) â€” expandir para inspecionar"):
            try:
                pretty = json.dumps(resp_obj, ensure_ascii=False, indent=2)
            except Exception:
                pretty = str(resp_obj)
            st.code(pretty[:20000], language="json")

st.markdown("---")
st.markdown(
    "**Aviso de Privacidade e Uso:** Coleta e tratamento de e-mails e nomes tem implicaÃ§Ãµes legais (LGPD/GDPR). "
    "Use estes dados apenas sob base legal apropriada, armazene com seguranÃ§a e permita remoÃ§Ã£o. "
    "Evite scraping massivo do LinkedIn; prefira ferramentas/licenciamento oficiais quando aplicÃ¡vel."
)
