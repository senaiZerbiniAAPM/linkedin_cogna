# streamlit_thordata_sourcing_with_emails.py
# -*- coding: utf-8 -*-
"""
Pesquisa de Perfis - Linkedin Â· Thordata SERP â€” integrado (consulta -> tabela -> tentativa de e-mail)
VersÃ£o com heurÃ­sticas avanÃ§adas para tentar localizar e-mails pÃºblicos:
 - multi-query (domÃ­nios prioritÃ¡rios)
 - renderizaÃ§Ã£o do perfil via Thordata (render_js) quando aplicÃ¡vel
 - download de pÃ¡ginas externas e extraÃ§Ã£o de e-mails do HTML
 - cache em memÃ³ria e persistente (./cache/email_cache.json)
 - botÃ£o "ForÃ§ar Enriquecimento (profundo)" para usar mais queries/render_js
AtenÃ§Ã£o: resultados nÃ£o garantidos; monitore uso/limites do Thordata; observe LGPD/GDPR.
"""
import os
import time
import json
import re
import hashlib
import html
from typing import Any, Dict, List, Optional, Tuple
from urllib.parse import urlparse
from pathlib import Path

import requests
import pandas as pd
import streamlit as st

st.set_page_config(page_title="Pesquisa de Perfis - Linkedin", layout="wide")

# ---------------- Config / endpoint ----------------
ENDPOINT = "https://scraperapi.thordata.com/request"

# ---------------- Settings (adjust as needed) ----------------
CACHE_PATH = Path("./cache/email_cache.json")
CACHE_PATH.parent.mkdir(parents=True, exist_ok=True)
PER_CANDIDATE_DELAY = 0.8  # seconds between sub-requests per candidate (tune with your quota)
DEFAULT_MAX_QUERIES = 8    # default number of query variants to attempt per candidate
DEEP_MAX_QUERIES = 20      # if "forÃ§ar enriquecimento profundo" selected
DEFAULT_RENDER_JS_FOR_PROFILE = True  # try render_js when opening profile link
USER_AGENT = "Mozilla/5.0 (compatible; ThordataBot/1.0)"

# ---------------- Utils / HeurÃ­sticas ----------------
EMAIL_RE = re.compile(r'[\w\.-]+@[\w\.-]+\.\w+', re.I)

def extract_name_from_linkedin_title(title: Optional[str]) -> Optional[str]:
    if not title or not isinstance(title, str):
        return None
    s = title.split("|")[0]
    s = s.split(" - ")[0].split(" â€” ")[0].strip()
    return s if s else None

def safe_json_load(obj: Any) -> Tuple[Optional[Dict], Optional[str]]:
    if isinstance(obj, dict):
        return obj, None
    if isinstance(obj, str):
        try:
            parsed = json.loads(obj)
            if isinstance(parsed, dict):
                return parsed, None
            return None, "JSON carregado nÃ£o Ã© um objeto dict (esperado)."
        except Exception as e:
            return None, f"Erro ao desserializar string JSON: {e}"
    return None, "Tipo de objeto inesperado; esperava dict ou str contendo JSON."

def find_organic_list(resp: Dict) -> List[Dict]:
    if not isinstance(resp, dict):
        return []
    for key in ("organic", "organic_results", "results", "items"):
        v = resp.get(key)
        if isinstance(v, list):
            return v
    v = resp.get("data")
    if isinstance(v, list):
        return v
    return []

def normalize_item_for_table(item: Dict) -> Dict[str, Optional[str]]:
    title = (item.get("title") or item.get("job_title") or "") if isinstance(item, dict) else ""
    link = item.get("link") or item.get("url") or item.get("source_url") or item.get("final_url") or None
    description = (item.get("description") or item.get("snippet") or "") if isinstance(item, dict) else ""

    name = extract_name_from_linkedin_title(title)
    if not name:
        source = item.get("source") or ""
        if isinstance(source, str) and "LinkedIn" in source:
            parts = re.split(r"Â·|-", source)
            if parts:
                candidate = parts[-1].strip()
                if candidate and len(candidate) > 1:
                    name = candidate
    if not name:
        m = re.search(r'\b([A-ZÃ€-Å¸][a-zÃ -Ã¿]+(?:\s+[A-ZÃ€-Å¸][a-zÃ -Ã¿]+){0,2})\b', title or "")
        if m:
            name = m.group(1)
    local_desc = description or ""
    return {
        "nome": name or "",
        "Link de perfil": link or "",
        # placeholder for Email (serÃ¡ preenchido depois)
        "Email": "",
        "Local e descriÃ§Ã£o": local_desc or ""
    }

def resp_to_table(resp_obj: Any, max_rows: int = 10) -> Tuple[pd.DataFrame, Optional[str]]:
    parsed, err = safe_json_load(resp_obj)
    if err:
        return pd.DataFrame(columns=["nome", "Link de perfil", "Email", "Local e descriÃ§Ã£o"]), err

    organic_list = find_organic_list(parsed)
    rows = []
    for item in organic_list:
        row = normalize_item_for_table(item)
        rows.append(row)

    if not rows:
        return pd.DataFrame(columns=["nome", "Link de perfil", "Email", "Local e descriÃ§Ã£o"]), None

    df = pd.DataFrame(rows)
    df = df.head(max_rows).reset_index(drop=True)
    return df, None

# ---------------- HTTP + Backoff (Thordata) ----------------
def exponential_backoff_sleep(attempt: int):
    wait = min(30, 2 ** attempt)
    time.sleep(wait)

def thordata_search(token: str,
                    q: str,
                    engine: str = "google",
                    domain: Optional[str] = None,
                    gl: Optional[str] = None,
                    hl: Optional[str] = None,
                    start: Optional[int] = None,
                    num: Optional[int] = None,
                    render_js: bool = False,
                    extra_params: Optional[Dict[str, Any]] = None,
                    max_retries: int = 6) -> Any:
    if not token:
        raise RuntimeError("Token nÃ£o informado. Defina THORDATA_TOKEN no ambiente ou cole na UI.")
    headers = {
        "Authorization": f"Bearer {token}",
        "Content-Type": "application/x-www-form-urlencoded"
    }
    data = {"engine": engine, "q": q, "json": "1"}
    if domain: data["domain"] = domain
    if gl: data["gl"] = gl
    if hl: data["hl"] = hl
    if start is not None: data["start"] = str(start)
    if num is not None: data["num"] = str(num)
    if render_js: data["render_js"] = "1"
    if extra_params:
        for k, v in (extra_params.items() if isinstance(extra_params, dict) else []):
            if v is not None:
                data[k] = str(v)

    attempt = 0
    while True:
        resp = requests.post(ENDPOINT, headers=headers, data=data, timeout=60)
        if resp.status_code == 200:
            try:
                return resp.json()
            except ValueError:
                return resp.text
        if resp.status_code == 429:
            if attempt >= max_retries:
                raise RuntimeError("Rate limited: excedeu tentativas (429).")
            exponential_backoff_sleep(attempt)
            attempt += 1
            continue
        if resp.status_code == 401:
            raise RuntimeError("401 Unauthorized - token invÃ¡lido ou expirado.")
        if resp.status_code == 402:
            raise RuntimeError("402 Payment Required - saldo insuficiente.")
        resp.raise_for_status()

# ---------------- Email search helpers (improved) ----------------
def _load_cache_persisted() -> Dict[str, str]:
    try:
        if CACHE_PATH.exists():
            with open(CACHE_PATH, "r", encoding="utf-8") as f:
                return json.load(f)
    except Exception:
        pass
    return {}

def _save_cache_persisted(cache: Dict[str, str]):
    try:
        with open(CACHE_PATH, "w", encoding="utf-8") as f:
            json.dump(cache, f, ensure_ascii=False, indent=2)
    except Exception:
        pass

def _cache_get(key: str):
    if "_email_cache" not in st.session_state:
        # initialize with persisted cache
        st.session_state["_email_cache"] = _load_cache_persisted()
    return st.session_state["_email_cache"].get(key)

def _cache_set(key: str, value):
    if "_email_cache" not in st.session_state:
        st.session_state["_email_cache"] = _load_cache_persisted()
    st.session_state["_email_cache"][key] = value or ""
    # persist to disk
    try:
        _save_cache_persisted(st.session_state["_email_cache"])
    except Exception:
        pass

def extract_emails_from_obj(obj: Any) -> List[str]:
    found = set()
    try:
        if isinstance(obj, str):
            for m in EMAIL_RE.findall(obj):
                found.add(m.lower())
            return list(found)
        if isinstance(obj, dict):
            for key in ("snippet", "description", "title", "content", "text"):
                v = obj.get(key)
                if isinstance(v, str):
                    for m in EMAIL_RE.findall(v):
                        found.add(m.lower())
            for k, v in obj.items():
                if isinstance(v, str):
                    for m in EMAIL_RE.findall(v):
                        found.add(m.lower())
                elif isinstance(v, list):
                    for item in v:
                        if isinstance(item, str):
                            for m in EMAIL_RE.findall(item):
                                found.add(m.lower())
                        elif isinstance(item, dict):
                            for kk in ("snippet","description","title","url","link"):
                                vv = item.get(kk)
                                if isinstance(vv, str):
                                    for m in EMAIL_RE.findall(vv):
                                        found.add(m.lower())
    except Exception:
        pass
    return list(found)

def try_fetch_page_and_extract_emails(url: str, token: str, render_js: bool = False) -> List[str]:
    """
    Tenta obter o HTML da URL e extrair e-mails.
    - Se for linkedin.com, tenta via Thordata com render_js (pode mostrar Contact info).
    - Para outros domÃ­nios, usa requests.get diretamente (com UA).
    Retorna lista de e-mails (possivelmente vazia).
    """
    emails = set()
    if not url:
        return []

    try:
        parsed = urlparse(url)
        domain = parsed.netloc.lower()
    except Exception:
        domain = ""

    try:
        if "linkedin.com" in domain:
            # usar thordata para abrir a URL com render_js (puxa JS)
            try:
                # q=url funciona para Thordata (ele renderiza a URL)
                resp = thordata_search(token=token, q=url, engine="google", render_js=True, num=1)
            except Exception:
                resp = None
            if resp:
                emails.update(extract_emails_from_obj(resp))
                organics = find_organic_list(resp) if isinstance(resp, dict) else []
                for item in organics:
                    for f in ("snippet","description","title","link","url"):
                        vv = item.get(f)
                        if isinstance(vv, str):
                            for m in EMAIL_RE.findall(vv):
                                emails.add(m.lower())
        else:
            headers = {"User-Agent": USER_AGENT}
            try:
                r = requests.get(url, timeout=15, headers=headers)
                if r.status_code == 200 and r.text:
                    text = html.unescape(r.text)
                    for m in EMAIL_RE.findall(text):
                        emails.add(m.lower())
            except Exception:
                pass
    except Exception:
        pass

    return sorted(emails)

def build_domain_priority_query(name: str, location: str = "") -> List[str]:
    qbase = []
    if name:
        qbase.append(f'"{name}"')
    if location:
        qbase.append(f'"{location}"')
    joined = " ".join(qbase).strip()

    queries = []
    if joined:
        queries.append(f'{joined} (email OR "e-mail" OR contato OR contato@ OR "@")')
        queries.append(f'(site:linkedin.com/in OR site:linkedin.com/pub) {joined} (contact OR "contato" OR email)')
        dev_sites = ["github.com", "gitlab.com", "github.io", "medium.com", "kaggle.com", "stackoverflow.com", "dev.to"]
        for ds in dev_sites:
            queries.append(f'site:{ds} {joined} (email OR contato OR "@")')
        queries.append(f'{joined} (site:.com OR site:.dev OR site:.site OR site:.me OR site:.blog) (email OR contato OR "@")')
        queries.append(f'(site:twitter.com OR site:facebook.com OR site:instagram.com) {joined} ("@" OR contato OR email)')
    else:
        queries.append('(email OR "e-mail" OR contato OR "@")')
    return queries

def find_email_for_name_v2(token: str,
                           name: str,
                           location: str = "",
                           profile_link: Optional[str] = None,
                           engine: str = "google",
                           domain: Optional[str] = None,
                           gl: Optional[str] = None,
                           hl: Optional[str] = None,
                           render_js: bool = False,
                           max_queries_per_candidate: int = DEFAULT_MAX_QUERIES) -> Optional[str]:
    """
    VersÃ£o melhorada de busca de e-mail:
    - usa mÃºltiplas queries priorizadas
    - tenta renderizar o prÃ³prio perfil no LinkedIn via Thordata
    - visita pÃ¡ginas externas e extrai e-mails do HTML
    - usa cache em st.session_state para nÃ£o repetir
    """
    if not name and not profile_link:
        return None

    key_src = f"{name}|{location}|{profile_link}"
    key = hashlib.sha1(key_src.encode("utf-8")).hexdigest()
    cached = _cache_get(key)
    if cached is not None:
        return cached or None

    found_emails: List[str] = []

    try:
        # 0) tenter renderizar o perfil (pode expor Contact info)
        if profile_link:
            try:
                emails = try_fetch_page_and_extract_emails(profile_link, token=token, render_js=DEFAULT_RENDER_JS_FOR_PROFILE)
                if emails:
                    found_emails.extend(emails)
            except Exception:
                pass
            time.sleep(PER_CANDIDATE_DELAY)

        # 1) queries multi-domÃ­nio
        queries = build_domain_priority_query(name, location)
        queries = queries[:max_queries_per_candidate]

        for q in queries:
            try:
                resp = thordata_search(token=token, q=q, engine=engine, domain=domain, gl=gl, hl=hl, render_js=render_js, num=10)
            except Exception:
                resp = None

            if resp:
                # extrair heurÃ­sticas do JSON
                if isinstance(resp, dict):
                    found_emails.extend(extract_emails_from_obj(resp))
                    organics = find_organic_list(resp)
                    for item in organics:
                        for f in ("snippet","description","title","link","url","source_url"):
                            vv = item.get(f)
                            if isinstance(vv, str):
                                for m in EMAIL_RE.findall(vv):
                                    found_emails.append(m.lower())
                        link = item.get("link") or item.get("url") or item.get("source_url")
                        if link and "linkedin.com" not in (link or "").lower():
                            try:
                                page_emails = try_fetch_page_and_extract_emails(link, token=token, render_js=False)
                                if page_emails:
                                    found_emails.extend(page_emails)
                            except Exception:
                                pass
                elif isinstance(resp, str):
                    for m in EMAIL_RE.findall(resp):
                        found_emails.append(m.lower())

            # dedupe and check
            found_emails = list(dict.fromkeys([e.lower() for e in found_emails if e]))
            if found_emails:
                _cache_set(key, found_emails[0])
                return found_emails[0]
            time.sleep(PER_CANDIDATE_DELAY)

    except Exception:
        pass

    _cache_set(key, "")
    return None

def enrich_df_with_emails(df: pd.DataFrame, token: str, engine: str, domain: Optional[str],
                          gl: Optional[str], hl: Optional[str], render_js: bool,
                          max_per_candidate: int = DEFAULT_MAX_QUERIES, force_deep: bool = False) -> pd.DataFrame:
    """
    Para cada linha do DataFrame, tenta encontrar e preencher a coluna 'Email'.
    Retorna novo DataFrame com a coluna atualizada.
    """
    if df is None or df.empty:
        return df
    if "Email" not in df.columns:
        df["Email"] = ""

    total = df.shape[0]
    progress = st.progress(0)
    for i, row in df.iterrows():
        if row.get("Email"):
            progress.progress(int((i+1)/total*100))
            continue
        name = (row.get("nome") or "").strip()
        local = (row.get("Local e descriÃ§Ã£o") or "")
        profile_link = row.get("Link de perfil") or ""
        # determine query depth
        depth = DEEP_MAX_QUERIES if force_deep else max_per_candidate
        try:
            email = find_email_for_name_v2(token=token, name=name, location=local, profile_link=profile_link,
                                           engine=engine, domain=domain, gl=gl, hl=hl, render_js=render_js,
                                           max_queries_per_candidate=depth)
        except Exception:
            email = None
        if email:
            df.at[i, "Email"] = email
        else:
            df.at[i, "Email"] = ""
        # polite delay
        time.sleep(PER_CANDIDATE_DELAY)
        progress.progress(int((i+1)/total*100))
    progress.empty()
    return df

# ---------------- UI ----------------
st.title("ðŸ”Ž   Pesquisa de Perfis - Linkedin")
st.markdown("Execute a busca; o resultado serÃ¡ automaticamente estruturado em tabela (nome, Link de perfil, Email, Local e descriÃ§Ã£o).")

with st.sidebar:
    st.header("ConfiguraÃ§Ãµes API / Query")
    env_token = os.getenv("THORDATA_TOKEN", "")
    api_token = st.text_input("TheirData API Key (Bearer)", value=env_token, type="password",
                              help="Recomendado: defina THORDATA_TOKEN no ambiente.")
    st.markdown("---")
    st.header("ParÃ¢metros padrÃ£o")
    engine = st.selectbox("Mecanismo", options=["google", "bing"], index=0)
    domain = st.selectbox("DomÃ­nio Google", options=["google.com", "google.com.br", "google.co.uk"], index=0)
    gl = st.selectbox("PaÃ­s (gl)", options=["BR", "US", "CA", "UK", ""], index=0)
    hl = st.selectbox("Idioma (hl)", options=["pt-BR", "en", "pt", ""], index=0)
    render_js = st.checkbox("Renderizar JS (mais lento/custoso)", value=False)
    st.markdown("---")
    do_extract = st.checkbox("Extrair nomes e e-mails automaticamente", value=True, key="do_extract_sidebar")
    st.markdown("Cache e limites")
    st.write(f"Cache persistente: `{CACHE_PATH.resolve()}`")
    st.write("Ajuste PER_CANDIDATE_DELAY / MAX_QUERIES no cÃ³digo (variÃ¡veis no topo) se necessÃ¡rio.")

with st.form("search_form"):
    st.subheader("Filtros de busca")
    area = st.selectbox("Ãrea (ex.:)", ["Data Science", "Software Engineering", "DevOps", "Security", "Product", "Design", "Sales/Marketing", "Outro"], index=0)
    competence = st.text_input("CompetÃªncia / skill (ex.: Python, AWS, Spark)", placeholder="python, aws, spark")
    location = st.text_input("Localidade (cidade / estado / paÃ­s)", placeholder="SÃ£o Paulo, Brazil")
    free_text = st.text_input("Termos adicionais (ex.: 'Bacharel', 'Mestrado', 'SÃªnior')", placeholder="")
    linkedin_only = st.checkbox("Somente LinkedIn (perfils) â€” site:linkedin.com/in", value=False,
                                help="Se marcado, a query serÃ¡ prefixada com site:linkedin.com/in OR site:linkedin.com/pub")
    per_page = st.slider("Resultados por pÃ¡gina (limite para tabela)", min_value=5, max_value=50, value=10, step=5)
    page_idx = st.number_input("PÃ¡gina (0 = primeira)", min_value=0, value=0, step=1)
    show_raw = st.checkbox("Mostrar JSON cru (apÃ³s consulta)", value=False)
    submitted = st.form_submit_button("ðŸ”Ž Pesquisar")

if "last_resp" not in st.session_state:
    st.session_state["last_resp"] = None
if "last_df" not in st.session_state:
    st.session_state["last_df"] = pd.DataFrame(columns=["nome", "Link de perfil", "Email", "Local e descriÃ§Ã£o"])

def build_query(area: str, competence: str, location: str, free_text: str, linkedin_only: bool) -> str:
    parts = []
    if area and area != "Outro":
        parts.append(area)
    if competence:
        parts.append(competence)
    if location:
        parts.append(location)
    if free_text:
        parts.append(free_text)
    q = " ".join(parts).strip()
    if linkedin_only:
        if q:
            q = f"(site:linkedin.com/in OR site:linkedin.com/pub) {q}"
        else:
            q = "(site:linkedin.com/in OR site:linkedin.com/pub)"
    return q

# search submit handling
if submitted:
    token_to_use = api_token.strip() or os.getenv("THORDATA_TOKEN", "").strip()
    if not token_to_use:
        st.error("Token nÃ£o fornecido. Defina THORDATA_TOKEN no ambiente ou cole a chave no campo da lateral.")
    else:
        q = build_query(area, competence, location, free_text, linkedin_only)
        if not q:
            st.warning("Query vazia â€” informe ao menos uma competÃªncia, Ã¡rea ou localidade.")
        else:
            start = page_idx * per_page
            with st.spinner("Consultando Thordata (SERP)..."):
                try:
                    resp_obj = thordata_search(token=token_to_use, q=q, engine=engine,
                                              domain=domain, gl=(gl or None), hl=(hl or None),
                                              start=start, num=per_page, render_js=render_js)
                except Exception as e:
                    st.error(f"Erro na busca: {e}")
                    resp_obj = None

            st.session_state["last_resp"] = resp_obj

            if resp_obj is not None:
                df_table, err = resp_to_table(resp_obj, max_rows=per_page)
                if err:
                    st.warning(err)
                st.session_state["last_df"] = df_table
            else:
                st.session_state["last_df"] = pd.DataFrame(columns=["nome", "Link de perfil", "Email", "Local e descriÃ§Ã£o"])

# area para colar JSON ou fazer upload (montar tabela sem consultar API)
st.markdown("---")
st.subheader("Ou: cole / carregue um JSON retornado pela API (opcional)")
col1, col2 = st.columns([3, 1])
with col1:
    pasted = st.text_area("Cole o JSON aqui (opcional)", height=140, placeholder='Cole aqui o JSON retornado pela API...')
with col2:
    upload = st.file_uploader("Ou faÃ§a upload do arquivo JSON", type=["json"])

if st.button("ðŸ”§ Montar tabela a partir do JSON colado/subido"):
    content = None
    if upload is not None:
        try:
            raw = upload.read()
            content = raw.decode("utf-8")
        except Exception as e:
            st.error(f"Erro lendo arquivo: {e}")
    elif pasted and pasted.strip():
        content = pasted.strip()

    if content:
        df_table, err = resp_to_table(content, max_rows=per_page)
        if err:
            st.warning(err)
        st.session_state["last_df"] = df_table
        parsed, jerr = safe_json_load(content)
        if parsed:
            st.session_state["last_resp"] = parsed
    else:
        st.info("Nenhum JSON fornecido para montar a tabela.")

# ----- Mostrar resultados estruturados (da Ãºltima resposta / upload) -----
st.markdown("---")
df_table = st.session_state.get("last_df", pd.DataFrame(columns=["nome", "Link de perfil", "Email", "Local e descriÃ§Ã£o"]))
count = int(df_table.shape[0]) if hasattr(df_table, "shape") else 0
st.markdown(f"### Resultados estruturados â€” {count} registros (mostrando atÃ© {per_page})")

if count == 0:
    st.info("Nenhum registro extraÃ­do para a tabela apÃ³s limpeza heurÃ­stica.")
else:
    token_to_use = api_token.strip() or os.getenv("THORDATA_TOKEN", "").strip()

    cols = st.columns([3, 2, 2])
    with cols[0]:
        st.write("OpÃ§Ãµes de enriquecimento:")
    with cols[1]:
        force_deep = st.checkbox("ForÃ§ar Enriquecimento (profundo) â€” mais queries/render_js", value=False)
    with cols[2]:
        max_queries_user = st.number_input("Max queries (por candidato)", min_value=1, max_value=100, value=DEFAULT_MAX_QUERIES, step=1)

    if do_extract and token_to_use:
        st.info("Tentando localizar e-mails pÃºblicos para os candidatos (pode consumir requisiÃ§Ãµes/saldo).")
        try:
            df_table = enrich_df_with_emails(df_table, token=token_to_use, engine=engine, domain=domain,
                                             gl=(gl or None), hl=(hl or None), render_js=render_js,
                                             max_per_candidate=int(max_queries_user), force_deep=bool(force_deep))
            st.session_state["last_df"] = df_table
        except Exception as e:
            st.warning(f"Erro ao buscar e-mails: {e}")

    # exibir tabela com colunas na ordem desejada (nome, link, email, local/descriÃ§Ã£o)
    display_df = df_table[["nome", "Link de perfil", "Email", "Local e descriÃ§Ã£o"]].copy()
    display_df["Local e descriÃ§Ã£o"] = display_df["Local e descriÃ§Ã£o"].astype(str).str.replace("\n", " ").str.slice(0, 500)
    st.dataframe(display_df, use_container_width=True)
    csv = display_df.to_csv(index=False).encode("utf-8")
    st.download_button("â¬‡ï¸ Exportar CSV (perfis)", csv, file_name="sourcing_perfis_with_emails.csv", mime="text/csv")

# mostrar links (clicÃ¡veis)
if count > 0:
    st.markdown("### Links (clique para abrir)")
    for _, r in df_table.head(per_page).iterrows():
        nome = r.get("nome") or "(sem nome)"
        link = r.get("Link de perfil") or ""
        local_desc = (r.get("Local e descriÃ§Ã£o") or "")[:200]
        email = r.get("Email") or ""
        if link:
            if email:
                st.write(f"- [{nome}]({link}) â€” {email} â€” {local_desc}")
            else:
                st.write(f"- [{nome}]({link}) â€” {local_desc}")
        else:
            if email:
                st.write(f"- {nome} â€” {email} â€” {local_desc}")
            else:
                st.write(f"- {nome} â€” {local_desc}")

# mostrar JSON cru se disponÃ­vel
if show_raw:
    resp_obj = st.session_state.get("last_resp")
    if resp_obj is None:
        st.info("Sem resposta em cache para exibir.")
    else:
        with st.expander("ðŸ”§ JSON cru (data retornada pela API) â€” expandir para inspecionar"):
            try:
                pretty = json.dumps(resp_obj, ensure_ascii=False, indent=2)
            except Exception:
                pretty = str(resp_obj)
            st.code(pretty[:20000], language="json")

st.markdown("---")
st.markdown(
    "**Aviso de Privacidade e Uso:** Coleta e tratamento de e-mails e nomes tem implicaÃ§Ãµes legais (LGPD/GDPR). "
    "Use estes dados apenas sob base legal apropriada, armazene com seguranÃ§a e permita remoÃ§Ã£o. "
    "Evite scraping massivo do LinkedIn; prefira ferramentas/licenciamento oficiais quando aplicÃ¡vel."
)
